{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e29990e1",
   "metadata": {},
   "source": [
    "**Zero Redundancy Optimizer（ZeRO）** 是现代大模型训练的核心技术之一，尤其是在分布式训练中非常重要\n",
    "---\n",
    "\n",
    "## 🚀 一、问题背景：为什么需要 ZeRO？\n",
    "\n",
    "在训练大模型（如 GPT、BERT、LLM）时，\n",
    "模型参数数量非常庞大（上亿、上百亿甚至上万亿），\n",
    "单个 GPU 无法同时容纳：\n",
    "\n",
    "* 模型参数（model weights）\n",
    "* 优化器状态（optimizer states）\n",
    "* 梯度（gradients）\n",
    "\n",
    "例如，一个 10B 参数的模型：\n",
    "\n",
    "> 仅优化器状态（如 Adam 的动量和方差）\n",
    "> 就可能占用 **参数的 4~8 倍显存！**\n",
    "\n",
    "→ 所以传统的 **Data Parallel (DP)** 已经无法负担。\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 二、ZeRO 的核心思想\n",
    "\n",
    "**ZeRO (Zero Redundancy Optimizer)** 的目标是：\n",
    "\n",
    "> **在多 GPU 上去除冗余的模型状态存储，让每个 GPU 只保存必要的一部分。**\n",
    "\n",
    "传统的 Data Parallel：\n",
    "\n",
    "* 每个 GPU 都保存一整份参数、梯度、优化器状态；\n",
    "* 很浪费显存（N 倍冗余）。\n",
    "\n",
    "ZeRO：\n",
    "\n",
    "* 把这些状态**分片（shard）**到不同 GPU；\n",
    "* 每个 GPU 只保存自己负责的那一部分；\n",
    "* 需要计算或更新时再彼此通信（AllGather/ReduceScatter）。\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 三、ZeRO 的三个阶段（Stage 1 / 2 / 3）\n",
    "\n",
    "| 阶段         | 优化器状态 | 梯度    | 参数    | 内存节省 | 通信代价 |\n",
    "| ---------- | ----- | ----- | ----- | ---- | ---- |\n",
    "| **ZeRO-1** | ✅ 分片  | ❌ 不分片 | ❌ 不分片 | 中等   | 低    |\n",
    "| **ZeRO-2** | ✅ 分片  | ✅ 分片  | ❌ 不分片 | 高    | 中    |\n",
    "| **ZeRO-3** | ✅ 分片  | ✅ 分片  | ✅ 分片  | 最高   | 高    |\n",
    "\n",
    "### ✅ ZeRO-1\n",
    "\n",
    "* 优化器状态分布在多个 GPU 上；\n",
    "* 每个 GPU 只保存一部分动量和方差；\n",
    "* 参数和梯度仍然是完整的。\n",
    "\n",
    "📉 显存节省：约 4×（Adam）的一部分。\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ ZeRO-2\n",
    "\n",
    "* 再进一步，把 **梯度** 也分片；\n",
    "* 所以反向传播时，梯度在计算完会直接分布式平均，不会在每个 GPU 上保存完整副本。\n",
    "\n",
    "📉 显存进一步下降，但需要更多通信（ReduceScatter）。\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ ZeRO-3\n",
    "\n",
    "* 最彻底的方案；\n",
    "* 连 **模型参数本身** 也分片，每个 GPU 只保存自己负责的那部分参数；\n",
    "* 在前向和反向时，临时 gather 需要的参数块。\n",
    "\n",
    "📉 显存使用最少，**可以训练万亿参数级模型**；\n",
    "📈 通信最频繁，但 DeepSpeed 做了高效调度优化。\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 四、架构类比（简单理解）\n",
    "\n",
    "假设模型参数有 12 个，\n",
    "传统 Data Parallel 每个 GPU 都保存全部：\n",
    "\n",
    "```\n",
    "GPU0: [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "GPU1: [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "GPU2: [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "```\n",
    "\n",
    "→ 3 倍冗余。\n",
    "\n",
    "ZeRO 把它们分片：\n",
    "\n",
    "```\n",
    "GPU0: [1,2,3,4]\n",
    "GPU1: [5,6,7,8]\n",
    "GPU2: [9,10,11,12]\n",
    "```\n",
    "\n",
    "→ 每个 GPU 只存 1/3 参数；\n",
    "→ 显存减少到原来的 1/3；\n",
    "→ 需要计算时再 AllGather。\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ 五、ZeRO 实现框架\n",
    "\n",
    "ZeRO 是 **DeepSpeed**（微软开源）的一部分，\n",
    "也被 **PyTorch Fully Sharded Data Parallel (FSDP)** 吸收进去了。\n",
    "\n",
    "| 框架          | 对应技术                          |\n",
    "| ----------- | ----------------------------- |\n",
    "| DeepSpeed   | ZeRO                          |\n",
    "| PyTorch     | FSDP（功能上类似 ZeRO）              |\n",
    "| Megatron-LM | ZeRO + Model Parallel 结合      |\n",
    "| ColossalAI  | Hybrid ZeRO + Tensor Parallel |\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 六、显存节省对比（Adam 优化器）\n",
    "\n",
    "假设模型参数大小为 `P`：\n",
    "\n",
    "| 组成部分         | 普通DP占用 | ZeRO-3占用 |\n",
    "| ------------ | ------ | -------- |\n",
    "| 参数           | P      | P / N    |\n",
    "| 梯度           | P      | P / N    |\n",
    "| 优化器状态（动量+方差） | 4P     | 4P / N   |\n",
    "| **总计**       | 6P     | 6P / N   |\n",
    "\n",
    "N 是 GPU 数量。\n",
    "→ 例如 8 张 GPU，可节省 87.5% 显存！\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 七、一句话总结\n",
    "\n",
    "> **ZeRO 通过在多 GPU 间分片参数、梯度和优化器状态，实现显存零冗余，从而让超大模型也能在有限显存上训练。**\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 附加说明：名字含义\n",
    "\n",
    "* “Zero” = “Zero Redundancy”（零冗余）\n",
    "* “Optimizer” = 它最初优化的对象是优化器状态（stage 1）\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
