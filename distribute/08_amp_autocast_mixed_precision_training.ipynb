{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108bf1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "import os#环境代理设置\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a732ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486.7002410888672\n"
     ]
    }
   ],
   "source": [
    "# all float32\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "print(model.get_memory_footprint() / (1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbb3b714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总参数量 : 124,439,808\n",
      "可训练量 : 124,439,808\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"总参数量 : {total_params:,}\")\n",
    "print(f\"可训练量 : {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d617e2",
   "metadata": {},
   "source": [
    "float32 一个参数占4字节\n",
    "\n",
    "0.1B 参数 => 0.4B 字节 => 486 兆字节\n",
    "\n",
    "$124,439,808*4 / 2^{20} = 486.7$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e42062a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249.3501205444336\n"
     ]
    }
   ],
   "source": [
    "# all float16\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2', torch_dtype=torch.float16)\n",
    "print(model.get_memory_footprint() / (1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776ae5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168.3501205444336\n"
     ]
    }
   ],
   "source": [
    "# torch.int8 \n",
    "# 混合精度量化\n",
    "# 自动跳过对精度敏感的小张量（bias、norm、embedding）\n",
    "# 只量化大权值矩阵（c_attn.weight, c_proj.weight, c_fc.weight）等\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2', load_in_8bit=True)\n",
    "print(model.get_memory_footprint() / (1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69af3009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16 transformer.wte.weight cuda:0\n",
      "torch.float16 transformer.wpe.weight cuda:0\n",
      "torch.float16 transformer.h.0.ln_1.weight cuda:0\n",
      "torch.float16 transformer.h.0.ln_1.bias cuda:0\n",
      "torch.int8 transformer.h.0.attn.c_attn.weight cuda:0\n",
      "torch.float16 transformer.h.0.attn.c_attn.bias cuda:0\n",
      "torch.int8 transformer.h.0.attn.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.0.attn.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.0.ln_2.weight cuda:0\n",
      "torch.float16 transformer.h.0.ln_2.bias cuda:0\n",
      "torch.int8 transformer.h.0.mlp.c_fc.weight cuda:0\n",
      "torch.float16 transformer.h.0.mlp.c_fc.bias cuda:0\n",
      "torch.int8 transformer.h.0.mlp.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.0.mlp.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.1.ln_1.weight cuda:0\n",
      "torch.float16 transformer.h.1.ln_1.bias cuda:0\n",
      "torch.int8 transformer.h.1.attn.c_attn.weight cuda:0\n",
      "torch.float16 transformer.h.1.attn.c_attn.bias cuda:0\n",
      "torch.int8 transformer.h.1.attn.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.1.attn.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.1.ln_2.weight cuda:0\n",
      "torch.float16 transformer.h.1.ln_2.bias cuda:0\n",
      "torch.int8 transformer.h.1.mlp.c_fc.weight cuda:0\n",
      "torch.float16 transformer.h.1.mlp.c_fc.bias cuda:0\n",
      "torch.int8 transformer.h.1.mlp.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.1.mlp.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.2.ln_1.weight cuda:0\n",
      "torch.float16 transformer.h.2.ln_1.bias cuda:0\n",
      "torch.int8 transformer.h.2.attn.c_attn.weight cuda:0\n",
      "torch.float16 transformer.h.2.attn.c_attn.bias cuda:0\n",
      "torch.int8 transformer.h.2.attn.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.2.attn.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.2.ln_2.weight cuda:0\n",
      "torch.float16 transformer.h.2.ln_2.bias cuda:0\n",
      "torch.int8 transformer.h.2.mlp.c_fc.weight cuda:0\n",
      "torch.float16 transformer.h.2.mlp.c_fc.bias cuda:0\n",
      "torch.int8 transformer.h.2.mlp.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.2.mlp.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.3.ln_1.weight cuda:0\n",
      "torch.float16 transformer.h.3.ln_1.bias cuda:0\n",
      "torch.int8 transformer.h.3.attn.c_attn.weight cuda:0\n",
      "torch.float16 transformer.h.3.attn.c_attn.bias cuda:0\n",
      "torch.int8 transformer.h.3.attn.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.3.attn.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.3.ln_2.weight cuda:0\n",
      "torch.float16 transformer.h.3.ln_2.bias cuda:0\n",
      "torch.int8 transformer.h.3.mlp.c_fc.weight cuda:0\n",
      "torch.float16 transformer.h.3.mlp.c_fc.bias cuda:0\n",
      "torch.int8 transformer.h.3.mlp.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.3.mlp.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.4.ln_1.weight cuda:0\n",
      "torch.float16 transformer.h.4.ln_1.bias cuda:0\n",
      "torch.int8 transformer.h.4.attn.c_attn.weight cuda:0\n",
      "torch.float16 transformer.h.4.attn.c_attn.bias cuda:0\n",
      "torch.int8 transformer.h.4.attn.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.4.attn.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.4.ln_2.weight cuda:0\n",
      "torch.float16 transformer.h.4.ln_2.bias cuda:0\n",
      "torch.int8 transformer.h.4.mlp.c_fc.weight cuda:0\n",
      "torch.float16 transformer.h.4.mlp.c_fc.bias cuda:0\n",
      "torch.int8 transformer.h.4.mlp.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.4.mlp.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.5.ln_1.weight cuda:0\n",
      "torch.float16 transformer.h.5.ln_1.bias cuda:0\n",
      "torch.int8 transformer.h.5.attn.c_attn.weight cuda:0\n",
      "torch.float16 transformer.h.5.attn.c_attn.bias cuda:0\n",
      "torch.int8 transformer.h.5.attn.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.5.attn.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.5.ln_2.weight cuda:0\n",
      "torch.float16 transformer.h.5.ln_2.bias cuda:0\n",
      "torch.int8 transformer.h.5.mlp.c_fc.weight cuda:0\n",
      "torch.float16 transformer.h.5.mlp.c_fc.bias cuda:0\n",
      "torch.int8 transformer.h.5.mlp.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.5.mlp.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.6.ln_1.weight cuda:0\n",
      "torch.float16 transformer.h.6.ln_1.bias cuda:0\n",
      "torch.int8 transformer.h.6.attn.c_attn.weight cuda:0\n",
      "torch.float16 transformer.h.6.attn.c_attn.bias cuda:0\n",
      "torch.int8 transformer.h.6.attn.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.6.attn.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.6.ln_2.weight cuda:0\n",
      "torch.float16 transformer.h.6.ln_2.bias cuda:0\n",
      "torch.int8 transformer.h.6.mlp.c_fc.weight cuda:0\n",
      "torch.float16 transformer.h.6.mlp.c_fc.bias cuda:0\n",
      "torch.int8 transformer.h.6.mlp.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.6.mlp.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.7.ln_1.weight cuda:0\n",
      "torch.float16 transformer.h.7.ln_1.bias cuda:0\n",
      "torch.int8 transformer.h.7.attn.c_attn.weight cuda:0\n",
      "torch.float16 transformer.h.7.attn.c_attn.bias cuda:0\n",
      "torch.int8 transformer.h.7.attn.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.7.attn.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.7.ln_2.weight cuda:0\n",
      "torch.float16 transformer.h.7.ln_2.bias cuda:0\n",
      "torch.int8 transformer.h.7.mlp.c_fc.weight cuda:0\n",
      "torch.float16 transformer.h.7.mlp.c_fc.bias cuda:0\n",
      "torch.int8 transformer.h.7.mlp.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.7.mlp.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.8.ln_1.weight cuda:0\n",
      "torch.float16 transformer.h.8.ln_1.bias cuda:0\n",
      "torch.int8 transformer.h.8.attn.c_attn.weight cuda:0\n",
      "torch.float16 transformer.h.8.attn.c_attn.bias cuda:0\n",
      "torch.int8 transformer.h.8.attn.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.8.attn.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.8.ln_2.weight cuda:0\n",
      "torch.float16 transformer.h.8.ln_2.bias cuda:0\n",
      "torch.int8 transformer.h.8.mlp.c_fc.weight cuda:0\n",
      "torch.float16 transformer.h.8.mlp.c_fc.bias cuda:0\n",
      "torch.int8 transformer.h.8.mlp.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.8.mlp.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.9.ln_1.weight cuda:0\n",
      "torch.float16 transformer.h.9.ln_1.bias cuda:0\n",
      "torch.int8 transformer.h.9.attn.c_attn.weight cuda:0\n",
      "torch.float16 transformer.h.9.attn.c_attn.bias cuda:0\n",
      "torch.int8 transformer.h.9.attn.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.9.attn.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.9.ln_2.weight cuda:0\n",
      "torch.float16 transformer.h.9.ln_2.bias cuda:0\n",
      "torch.int8 transformer.h.9.mlp.c_fc.weight cuda:0\n",
      "torch.float16 transformer.h.9.mlp.c_fc.bias cuda:0\n",
      "torch.int8 transformer.h.9.mlp.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.9.mlp.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.10.ln_1.weight cuda:0\n",
      "torch.float16 transformer.h.10.ln_1.bias cuda:0\n",
      "torch.int8 transformer.h.10.attn.c_attn.weight cuda:0\n",
      "torch.float16 transformer.h.10.attn.c_attn.bias cuda:0\n",
      "torch.int8 transformer.h.10.attn.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.10.attn.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.10.ln_2.weight cuda:0\n",
      "torch.float16 transformer.h.10.ln_2.bias cuda:0\n",
      "torch.int8 transformer.h.10.mlp.c_fc.weight cuda:0\n",
      "torch.float16 transformer.h.10.mlp.c_fc.bias cuda:0\n",
      "torch.int8 transformer.h.10.mlp.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.10.mlp.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.11.ln_1.weight cuda:0\n",
      "torch.float16 transformer.h.11.ln_1.bias cuda:0\n",
      "torch.int8 transformer.h.11.attn.c_attn.weight cuda:0\n",
      "torch.float16 transformer.h.11.attn.c_attn.bias cuda:0\n",
      "torch.int8 transformer.h.11.attn.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.11.attn.c_proj.bias cuda:0\n",
      "torch.float16 transformer.h.11.ln_2.weight cuda:0\n",
      "torch.float16 transformer.h.11.ln_2.bias cuda:0\n",
      "torch.int8 transformer.h.11.mlp.c_fc.weight cuda:0\n",
      "torch.float16 transformer.h.11.mlp.c_fc.bias cuda:0\n",
      "torch.int8 transformer.h.11.mlp.c_proj.weight cuda:0\n",
      "torch.float16 transformer.h.11.mlp.c_proj.bias cuda:0\n",
      "torch.float16 transformer.ln_f.weight cuda:0\n",
      "torch.float16 transformer.ln_f.bias cuda:0\n"
     ]
    }
   ],
   "source": [
    "for name, para in model.named_parameters():\n",
    "    print(para.dtype, name, para.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d012bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140604414518752\n",
      "140604414518752\n"
     ]
    }
   ],
   "source": [
    "print(id(model.lm_head.weight))        \n",
    "print(id(model.transformer.wte.weight)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c32af032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Linear8bitLt(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f34c008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af34d96e",
   "metadata": {},
   "source": [
    "- transformer.wte.weight、transformer.wpe.weight：torch.float16\n",
    "- h.0 - h.11\n",
    "    - ln_1.weight, ln_1.bias, ln_2.weight, ln_2.bias: torch.float16\n",
    "    - attn\n",
    "        - c_attn.weight: torch.int8\n",
    "            - bias: torch.float16\n",
    "        - c_proj.weight: torch.int8\n",
    "            - bias: torch.float16\n",
    "    - mlp\n",
    "        - c_fc.weight: torch.int8\n",
    "            - bias: torch.float16\n",
    "    - ln_f.weight, ln_f.bias: torch.float16\n",
    "\n",
    "lm_head 权重与 token embedding 权重 在 GPT-2 里**共享同一矩阵**\n",
    "结构图里虽然出现 lm_head，但它 不是独立参数，而是 transformer.wte.weight 的 引用（alias），所以 named_parameters() 自动去重只列一次"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d4754a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a201353b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.cuda.amp.autocast_mode.autocast"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.amp.autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4e4bd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in autocast torch.float16 cuda:0\n",
      "in autocast torch.float16 cuda:0\n",
      "out autocast torch.float32 cuda:0\n",
      "out autocast torch.float16 cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_614184/429421949.py:7: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(): #开启「自动混合精度」环境\n"
     ]
    }
   ],
   "source": [
    "# Creates some tensors in default dtype (here assumed to be float32)\n",
    "a_float32 = torch.rand((8, 8), device=\"cuda\")\n",
    "b_float32 = torch.rand((8, 8), device=\"cuda\")\n",
    "c_float32 = torch.rand((8, 8), device=\"cuda\")\n",
    "d_float32 = torch.rand((8, 8), device=\"cuda\")\n",
    "\n",
    "with torch.cuda.amp.autocast(): #开启「自动混合精度」环境\n",
    "    # 在这个环境里，PyTorch 会自动把合适的算子（如矩阵乘法、卷积）切换到 float16（半精度）执行，\n",
    "    # 而一些对精度敏感的算子（如归一化、softmax）依然保持 float32。\n",
    "    e = torch.mm(a_float32, b_float32)\n",
    "    print('in autocast', e.dtype, e.device)\n",
    "\n",
    "    f = torch.mm(d_float32, e)\n",
    "    print('in autocast', f.dtype, e.device)\n",
    "\n",
    "# 退出autocast后, 显示调用.float() 使用 float32\n",
    "g_float32 = torch.mm(d_float32, f.float())\n",
    "print('out autocast', g_float32.dtype, g_float32.device)\n",
    "\n",
    "h_float16 = torch.mm(d_float32.half(), f)\n",
    "print('out autocast', h_float16.dtype, h_float16.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5da1cad",
   "metadata": {},
   "source": [
    "## 前置知识"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08abff8f",
   "metadata": {},
   "source": [
    "#### ✅ FP16（半精度）优势：\n",
    "- **动态范围足够**：虽然FP16表示范围比FP32小，但在大多数深度学习计算中是够用的。\n",
    "- **计算快**：FP16的计算吞吐量通常是FP32的**8倍**。\n",
    "- **内存效率高**：\n",
    "  - 内存带宽提升**2倍**（因为数据更小，传输更快）。\n",
    "  - 显存占用减半（**1/2x**），可以训练更大的模型或batch size。\n",
    "\n",
    "#### ⚠️ 但FP16也有问题：\n",
    "- **精度不足**：某些操作（如累加、指数运算、权重更新）在FP16下容易**下溢（underflow）**或**精度不够**。\n",
    "  - 举例：`0.1 + 0.0001` 在FP16下可能直接等于`0.1`，因为`0.0001`被“吃掉”了。\n",
    "  - 权重更新时，如果更新量太小（比如小于 \\(2^{-11} \\approx 0.00049\\)），FP16直接忽略，**模型不学习**。\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 所以需要混合精度（Mixed Precision）：\n",
    "- **用FP16做大部分计算**（快、省内存）。\n",
    "- **用FP32做关键操作**（保证精度）：\n",
    "  - **累加（reductions）**\n",
    "  - **指数运算（exponentiation）**\n",
    "  - **权重更新（weight updates）**\n",
    "\n",
    "---\n",
    "\n",
    "> **FP16快但“粗心”，FP32慢但“细心”，混合精度训练就是让他俩分工合作，既快又准。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "727ac527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0002], device='cuda:0', dtype=torch.float16)\n",
      "tensor([1.], device='cuda:0', dtype=torch.float16)\n",
      "tensor([True], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.cuda.HalfTensor([2**-12])\n",
    "one = torch.cuda.HalfTensor([1.0])\n",
    "print(a)              # tensor([5.9605e-08], dtype=torch.float16)\n",
    "print(one + a)        # tensor([1.], dtype=torch.float16)\n",
    "print(one + a == one) # tensor([True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61fdeb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0002], device='cuda:0')\n",
      "tensor([1.0002], device='cuda:0')\n",
      "tensor([False], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([2**-12], dtype=torch.float32, device='cuda')\n",
    "one = torch.tensor([1.0], dtype=torch.float32, device='cuda')\n",
    "print(a)              # tensor([5.9605e-08], device='cuda')\n",
    "print(one + a)        # tensor([1.0000001], device='cuda')\n",
    "print(one + a == one) # tensor([False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8693677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4096*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87545497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(inf, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.float16\n",
    "a = torch.cuda.HalfTensor(4096)\n",
    "a.fill_(16)\n",
    "a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "803f3736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(65536., device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.float32\n",
    "a = torch.cuda.FloatTensor(4096)\n",
    "a.fill_(16)\n",
    "a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c27f4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = torch.cuda.HalfTensor([1.])\n",
    "update = torch.cuda.HalfTensor([.0001])\n",
    "para + update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bbfc1e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0010], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = torch.cuda.HalfTensor([1.])\n",
    "update = torch.cuda.HalfTensor([.001])\n",
    "para + update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ca4b10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0001], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = torch.cuda.FloatTensor([1.])\n",
    "update = torch.cuda.FloatTensor([.0001])\n",
    "para + update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff2c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "para = torch.cuda.FloatTensor([1.])\n",
    "update = torch.cuda.FloatTensor([.0001])\n",
    "para + update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b88c31",
   "metadata": {},
   "source": [
    "混合精度训练的核心循环：\n",
    "- FP16 跑前向+反向（快）；\n",
    "- FP32 做梯度累加与权重更新（准）；\n",
    "- 再把权重压回 FP16 继续下一轮。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aa2930",
   "metadata": {},
   "source": [
    "### loss scaling\n",
    "\n",
    "``` python\n",
    "scaler = GradScaler()\n",
    "\n",
    "# forward\n",
    "with autocast():\n",
    "    output = model(input)\n",
    "    loss = loss_fn(output, target)\n",
    "\n",
    "# backward\n",
    "scaler.sacle(loss).backward()\n",
    "scaler.step(optimizer)\n",
    "scaler.update()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6e2233",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### master-weights-scale\n",
    "\n",
    "``` python\n",
    "# 计算梯度\n",
    "loss.backward()\n",
    "\n",
    "# 将计算的梯度从float16模型复制到float32模型\n",
    "for param, param_float32 in zip(model.parameters(), model_float32.parameters()):\n",
    "    if param.grad is not None:\n",
    "        param_float32.grad = param.grad.float() * scale_factor  # 应用梯度缩放\n",
    "\n",
    "# 更新主权重（float32模型）\n",
    "optimizer.step()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTroch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
