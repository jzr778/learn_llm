{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "286f367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20d747",
   "metadata": {},
   "source": [
    "`dist.init_process_group`\n",
    "\n",
    "``` python\n",
    "def init_process(rank, size, backend='nccl'):\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank, world_size=size)\n",
    "```\n",
    "\n",
    "- 分布式进程组（distributed progress group）\n",
    "    - 主节点：master node；localhost就是本机；\n",
    "    - MASTER_ADDR/MASTER_PORT: 设置主节点的地址及端口号，主要用于分布式的管理；\n",
    "    - 哪怕是单机（单节点）多卡，也要显示设置主节点；虽然它们都在同一台机器上，但逻辑上需要一个“中心协调者”，进程间需要通过主节点完成握手\n",
    "- 单机双卡，2个进程（Processes），每个进程都会调用 init_process 来初始化分布式环境；\n",
    "\n",
    "- 进程间通信的后端：communication backend\n",
    "    - NCCL: NVIDIA Collective Communication Library，仅限NVIDIA GPU\n",
    "    - Gloo: Facebook自研，CPU/GPU\n",
    "    - MPI\n",
    "\n",
    "| Backend | 支持设备     | 支持通信操作                              | 支持 Float32 | 支持 Float16 | 备注 |\n",
    "|---------|--------------|--------------------------------------------|--------------|--------------|------|\n",
    "| MPI     | CPU, GPU     | 所有操作（All）                            | 是           | 否           | 需系统预装 MPI（如 OpenMPI） |\n",
    "| GLOO    | CPU（全）<br>GPU（部分：broadcast, all-reduce） | CPU：所有<br>GPU：broadcast, all-reduce, reduce, all-gather | 是           | 是           | PyTorch 内置，适合 CPU 或调试 |\n",
    "| NCCL    | GPU only     | 所有操作（All）                            | 是           | 是           | PyTorch 内置，GPU 训练首选 |\n",
    "\n",
    "总结：\n",
    "| 场景               | 推荐后端                      |\n",
    "| ---------------- | ------------------------- |\n",
    "| 单机/多机 **GPU 训练** | ✅ `nccl`（速度最快，支持 Float16） |\n",
    "| CPU 训练或调试        | ✅ `gloo`（内置，跨平台）          |\n",
    "| 超算/已有 MPI 集群     | ✅ `mpi`（需自己编译安装）          |\n",
    "\n",
    "\n",
    "- 后端（backend）决定“用什么语言说话”\n",
    "- 初始化方法（init_method）决定“第一次怎么互相找到对方”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3175cae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Point-to-Point Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f0c60",
   "metadata": {},
   "source": [
    "- send & recv\n",
    "    - send 和 recv 是 **点对点阻塞式** 操作；\n",
    "    - 只有配对的 send 与 recv 都到达后，数据才真正完成传输，程序才会继续往下走。\n",
    "\n",
    "``` python \n",
    "def run(rank, size):\n",
    "    torch.cuda.set_device(rank)\n",
    "    tensor = torch.zeros(1).to(rank)\n",
    "    if rank == 0:\n",
    "        tensor += 1\n",
    "        dist.send(tensor=tensor, dst=1)\n",
    "    else:\n",
    "        print('init tensor',tensor)\n",
    "        dist.recv(tensor=tensor, src=0)\n",
    "    print(f'Rank: {rank}, has data {tensor}')\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "``` text\n",
    "init tentor tensor([0.], device='cuda:1')\n",
    "Rank: 1, has data tensor([1.], device='cuda:1')\n",
    "Rank: 0, has data tensor([1.], device='cuda:0')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bd4fd0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Collective Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240aefb7",
   "metadata": {},
   "source": [
    "``` python\n",
    "\"\"\"All-Reduce example\"\"\"\n",
    "def run(rank, size):\n",
    "    group = dist.new_group([0,1])\n",
    "    if rank == 0:\n",
    "        tensor = torch.tensor([1.,2.,3.])\n",
    "    else:\n",
    "        tensor = torch.tensor([4.,5.,6.])\n",
    "    tensor = tensor.to(rank)\n",
    "    print(f'Rank: {rank}, random tensor: {tensor}')\n",
    "    dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group)\n",
    "    print(f'Rank: {rank}, has data: {tensor}')\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "``` text\n",
    "Rank: 1, random tensor: tensor([4., 5., 6.], device='cuda:1')\n",
    "Rank: 0, random tensor: tensor([1., 2., 3.], device='cuda:0')\n",
    "Rank: 1, has data: tensor([5., 7., 9.], device='cuda:1')\n",
    "Rank: 0, has data: tensor([5., 7., 9.], device='cuda:0')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e698d372",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "多进程管理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec14ee40",
   "metadata": {},
   "source": [
    "``` python\n",
    "if __name__ == \"__main__\":\n",
    "    size = 2\n",
    "    processes = []\n",
    "    mp.set_start_method(\"spawn\")\n",
    "    for rank in range(size):\n",
    "        p = mp.Process(target=init_process, args=(rank, size, run))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    \n",
    "    for p in processes:\n",
    "        p.join()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60af3d86",
   "metadata": {},
   "source": [
    "- 通过`mp.set_start_method(\"spawn\")`设置进程的启动方式为`spawn`\n",
    "    - `spawn`方式会为每个子进程创建一个全新的Python解释器进程\n",
    "    - `mp.get_start_method()` => `fork`（ubuntu system）\n",
    "- 通过在一个循环中对所有进程调用 join() 方法，主进程会等待所有子进程执行完成后再继续执行。\n",
    "    - join()主进程阻塞，等待所有子进程完成它们的任务，这是确保数据完整性和避免竞争条件的重要机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5091a0",
   "metadata": {},
   "source": [
    "| 参数 | 作用 |\n",
    "|---|---|\n",
    "| `Process` | **工厂**：生成一条**新的操作系统进程**。 |\n",
    "| `target` | **订单**：告诉新进程“你一生下来就执行哪个函数”。 |\n",
    "\n",
    "---\n",
    "\n",
    "### 1. `mp.Process`\n",
    "- 是 `multiprocessing` 的**进程类**。  \n",
    "- 调用 `Process(...)` **并不开始执行**，只是**把“待办任务”打包成对象**；  \n",
    "- 必须再调用 `.start()` 才会真正 fork/spawn 出一个**独立 Python 解释器进程**。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `target`\n",
    "- 必须是**可调用对象**（函数 / 类 `__call__`）。  \n",
    "- 新进程启动后，**入口就等价于**  \n",
    "  ```python\n",
    "  target(*args, **kwargs)\n",
    "  ```\n",
    "  在你给的代码里就是  \n",
    "  ```python\n",
    "  init_process(rank, size, run)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 类比\n",
    "把 `Process` 想成“快递单”：  \n",
    "- `target` 写**收货地址**（函数），  \n",
    "- `args` 写**包裹内容**（参数），  \n",
    "- `.start()` 相当于“发货”——系统立刻派一辆新车（子进程）把包裹送到目的地并执行。\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 一句话总结\n",
    "`Process` 负责**生**进程；  \n",
    "`target` 负责告诉新生进程**“你活着的第一件事就是跑这个函数”**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e313e90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.zeros(1)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTroch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
