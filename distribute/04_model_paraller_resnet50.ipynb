{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b38c11",
   "metadata": {},
   "source": [
    "outline\n",
    "\n",
    "- 数据并行是切数据（scattering inputs and gathering outputs），模型并行是切模型（shards）\n",
    "    - 模型并行：单卡放不下一份模型；\n",
    "    - 将一份大模型，不同的层切分到不同的卡上\n",
    "- device_map：Huggingface\n",
    "- 模型并行 on ToyModel\n",
    "- 模型并行：on ResNet\n",
    "- 不需要引入额外的 torch api 支持；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7822a30d",
   "metadata": {},
   "source": [
    "`device_map` 是 Hugging Face Transformers 库中的一个参数，主要用于控制模型在多设备（如多块GPU）环境下的负载分配。当加载大型模型进行推理或训练时，可以通过 `device_map` 将模型的不同层自动或手动分配到不同的计算设备上，以实现资源的高效利用和并行计算。\n",
    "主要功能与用法：\n",
    "- 自动分配：  \n",
    "  设置 `device_map=\"auto\"` 时，Transformers 会结合 Accelerate 库自动将模型各层分配到可用设备上，优先使用 GPU 显存，若显存不足则使用 CPU 内存或磁盘空间。\n",
    "  \n",
    "- 多种分配策略：  \n",
    "  - `\"auto\"` / `\"balanced\"`：在所有 GPU 上均匀分配模型层。  \n",
    "  - `\"balanced_low_0\"`：除第一个 GPU 外，其他 GPU 均衡分配，第一个 GPU 占用较少资源，适合需要在 GPU 0 上执行生成任务（如 `generate`）的场景。  \n",
    "  - `\"sequential\"`：按顺序分配，优先填满 GPU 0，再依次向后分配。\n",
    "- 手动配置：  \n",
    "  可自定义 `device_map` 为字典形式，指定模型的每个模块分配到哪个设备（如 `\"cuda:0\"`、`\"cuda:1\"` 或 `\"cpu\"`），适用于精细控制模型部署场景。\n",
    "- 与量化结合使用：  \n",
    "  在使用 4 位或 8 位量化（如 BitsAndBytes）时，`device_map=\"auto\"` 可让模型在低显存设备上运行，通过将部分层卸载到 CPU 或磁盘来节省显存。\n",
    "典型应用场景：\n",
    "- 单机多卡推理，提升大模型推理速度。\n",
    "- 显存不足时，通过 CPU 或磁盘扩展内存，实现大模型加载。\n",
    "- 多任务并行推理，充分利用多设备资源。\n",
    "简而言之，`device_map` 是实现模型并行、资源优化和高效推理的关键工具，特别适用于大模型在多设备环境下的部署。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5446ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os#环境代理设置\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb120006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d838bbfccd06432085d690ae1fe9e691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
    "model = LlamaForCausalLM.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\",\n",
    "                                         load_in_8bit=True,\n",
    "                                         device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "322bfaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,\t model.embed_tokens.weight, \t cuda:0 \t torch.float16\n",
      "1,\t model.layers.0.self_attn.q_proj.weight, \t cuda:0 \t torch.int8\n",
      "2,\t model.layers.0.self_attn.k_proj.weight, \t cuda:0 \t torch.int8\n",
      "3,\t model.layers.0.self_attn.v_proj.weight, \t cuda:0 \t torch.int8\n",
      "4,\t model.layers.0.self_attn.o_proj.weight, \t cuda:0 \t torch.int8\n",
      "5,\t model.layers.0.mlp.gate_proj.weight, \t cuda:0 \t torch.int8\n",
      "6,\t model.layers.0.mlp.up_proj.weight, \t cuda:0 \t torch.int8\n",
      "7,\t model.layers.0.mlp.down_proj.weight, \t cuda:0 \t torch.int8\n",
      "8,\t model.layers.0.input_layernorm.weight, \t cuda:0 \t torch.float16\n",
      "9,\t model.layers.0.post_attention_layernorm.weight, \t cuda:0 \t torch.float16\n",
      "10,\t model.layers.1.self_attn.q_proj.weight, \t cuda:0 \t torch.int8\n",
      "11,\t model.layers.1.self_attn.k_proj.weight, \t cuda:0 \t torch.int8\n",
      "12,\t model.layers.1.self_attn.v_proj.weight, \t cuda:0 \t torch.int8\n",
      "13,\t model.layers.1.self_attn.o_proj.weight, \t cuda:0 \t torch.int8\n",
      "14,\t model.layers.1.mlp.gate_proj.weight, \t cuda:0 \t torch.int8\n",
      "15,\t model.layers.1.mlp.up_proj.weight, \t cuda:0 \t torch.int8\n",
      "16,\t model.layers.1.mlp.down_proj.weight, \t cuda:0 \t torch.int8\n",
      "17,\t model.layers.1.input_layernorm.weight, \t cuda:0 \t torch.float16\n",
      "18,\t model.layers.1.post_attention_layernorm.weight, \t cuda:0 \t torch.float16\n",
      "19,\t model.layers.2.self_attn.q_proj.weight, \t cuda:1 \t torch.int8\n",
      "20,\t model.layers.2.self_attn.k_proj.weight, \t cuda:1 \t torch.int8\n",
      "21,\t model.layers.2.self_attn.v_proj.weight, \t cuda:1 \t torch.int8\n",
      "22,\t model.layers.2.self_attn.o_proj.weight, \t cuda:1 \t torch.int8\n",
      "23,\t model.layers.2.mlp.gate_proj.weight, \t cuda:1 \t torch.int8\n",
      "24,\t model.layers.2.mlp.up_proj.weight, \t cuda:1 \t torch.int8\n",
      "25,\t model.layers.2.mlp.down_proj.weight, \t cuda:1 \t torch.int8\n",
      "26,\t model.layers.2.input_layernorm.weight, \t cuda:1 \t torch.float16\n",
      "27,\t model.layers.2.post_attention_layernorm.weight, \t cuda:1 \t torch.float16\n",
      "28,\t model.layers.3.self_attn.q_proj.weight, \t cuda:1 \t torch.int8\n",
      "29,\t model.layers.3.self_attn.k_proj.weight, \t cuda:1 \t torch.int8\n",
      "30,\t model.layers.3.self_attn.v_proj.weight, \t cuda:1 \t torch.int8\n",
      "31,\t model.layers.3.self_attn.o_proj.weight, \t cuda:1 \t torch.int8\n",
      "32,\t model.layers.3.mlp.gate_proj.weight, \t cuda:1 \t torch.int8\n",
      "33,\t model.layers.3.mlp.up_proj.weight, \t cuda:1 \t torch.int8\n",
      "34,\t model.layers.3.mlp.down_proj.weight, \t cuda:1 \t torch.int8\n",
      "35,\t model.layers.3.input_layernorm.weight, \t cuda:1 \t torch.float16\n",
      "36,\t model.layers.3.post_attention_layernorm.weight, \t cuda:1 \t torch.float16\n",
      "37,\t model.layers.4.self_attn.q_proj.weight, \t cuda:1 \t torch.int8\n",
      "38,\t model.layers.4.self_attn.k_proj.weight, \t cuda:1 \t torch.int8\n",
      "39,\t model.layers.4.self_attn.v_proj.weight, \t cuda:1 \t torch.int8\n",
      "40,\t model.layers.4.self_attn.o_proj.weight, \t cuda:1 \t torch.int8\n",
      "41,\t model.layers.4.mlp.gate_proj.weight, \t cuda:1 \t torch.int8\n",
      "42,\t model.layers.4.mlp.up_proj.weight, \t cuda:1 \t torch.int8\n",
      "43,\t model.layers.4.mlp.down_proj.weight, \t cuda:1 \t torch.int8\n",
      "44,\t model.layers.4.input_layernorm.weight, \t cuda:1 \t torch.float16\n",
      "45,\t model.layers.4.post_attention_layernorm.weight, \t cuda:1 \t torch.float16\n",
      "46,\t model.layers.5.self_attn.q_proj.weight, \t cuda:1 \t torch.int8\n",
      "47,\t model.layers.5.self_attn.k_proj.weight, \t cuda:1 \t torch.int8\n",
      "48,\t model.layers.5.self_attn.v_proj.weight, \t cuda:1 \t torch.int8\n",
      "49,\t model.layers.5.self_attn.o_proj.weight, \t cuda:1 \t torch.int8\n",
      "50,\t model.layers.5.mlp.gate_proj.weight, \t cuda:1 \t torch.int8\n",
      "51,\t model.layers.5.mlp.up_proj.weight, \t cuda:1 \t torch.int8\n",
      "52,\t model.layers.5.mlp.down_proj.weight, \t cuda:1 \t torch.int8\n",
      "53,\t model.layers.5.input_layernorm.weight, \t cuda:1 \t torch.float16\n",
      "54,\t model.layers.5.post_attention_layernorm.weight, \t cuda:1 \t torch.float16\n",
      "55,\t model.layers.6.self_attn.q_proj.weight, \t cuda:2 \t torch.int8\n",
      "56,\t model.layers.6.self_attn.k_proj.weight, \t cuda:2 \t torch.int8\n",
      "57,\t model.layers.6.self_attn.v_proj.weight, \t cuda:2 \t torch.int8\n",
      "58,\t model.layers.6.self_attn.o_proj.weight, \t cuda:2 \t torch.int8\n",
      "59,\t model.layers.6.mlp.gate_proj.weight, \t cuda:2 \t torch.int8\n",
      "60,\t model.layers.6.mlp.up_proj.weight, \t cuda:2 \t torch.int8\n",
      "61,\t model.layers.6.mlp.down_proj.weight, \t cuda:2 \t torch.int8\n",
      "62,\t model.layers.6.input_layernorm.weight, \t cuda:2 \t torch.float16\n",
      "63,\t model.layers.6.post_attention_layernorm.weight, \t cuda:2 \t torch.float16\n",
      "64,\t model.layers.7.self_attn.q_proj.weight, \t cuda:2 \t torch.int8\n",
      "65,\t model.layers.7.self_attn.k_proj.weight, \t cuda:2 \t torch.int8\n",
      "66,\t model.layers.7.self_attn.v_proj.weight, \t cuda:2 \t torch.int8\n",
      "67,\t model.layers.7.self_attn.o_proj.weight, \t cuda:2 \t torch.int8\n",
      "68,\t model.layers.7.mlp.gate_proj.weight, \t cuda:2 \t torch.int8\n",
      "69,\t model.layers.7.mlp.up_proj.weight, \t cuda:2 \t torch.int8\n",
      "70,\t model.layers.7.mlp.down_proj.weight, \t cuda:2 \t torch.int8\n",
      "71,\t model.layers.7.input_layernorm.weight, \t cuda:2 \t torch.float16\n",
      "72,\t model.layers.7.post_attention_layernorm.weight, \t cuda:2 \t torch.float16\n",
      "73,\t model.layers.8.self_attn.q_proj.weight, \t cuda:2 \t torch.int8\n",
      "74,\t model.layers.8.self_attn.k_proj.weight, \t cuda:2 \t torch.int8\n",
      "75,\t model.layers.8.self_attn.v_proj.weight, \t cuda:2 \t torch.int8\n",
      "76,\t model.layers.8.self_attn.o_proj.weight, \t cuda:2 \t torch.int8\n",
      "77,\t model.layers.8.mlp.gate_proj.weight, \t cuda:2 \t torch.int8\n",
      "78,\t model.layers.8.mlp.up_proj.weight, \t cuda:2 \t torch.int8\n",
      "79,\t model.layers.8.mlp.down_proj.weight, \t cuda:2 \t torch.int8\n",
      "80,\t model.layers.8.input_layernorm.weight, \t cuda:2 \t torch.float16\n",
      "81,\t model.layers.8.post_attention_layernorm.weight, \t cuda:2 \t torch.float16\n",
      "82,\t model.layers.9.self_attn.q_proj.weight, \t cuda:2 \t torch.int8\n",
      "83,\t model.layers.9.self_attn.k_proj.weight, \t cuda:2 \t torch.int8\n",
      "84,\t model.layers.9.self_attn.v_proj.weight, \t cuda:2 \t torch.int8\n",
      "85,\t model.layers.9.self_attn.o_proj.weight, \t cuda:2 \t torch.int8\n",
      "86,\t model.layers.9.mlp.gate_proj.weight, \t cuda:2 \t torch.int8\n",
      "87,\t model.layers.9.mlp.up_proj.weight, \t cuda:2 \t torch.int8\n",
      "88,\t model.layers.9.mlp.down_proj.weight, \t cuda:2 \t torch.int8\n",
      "89,\t model.layers.9.input_layernorm.weight, \t cuda:2 \t torch.float16\n",
      "90,\t model.layers.9.post_attention_layernorm.weight, \t cuda:2 \t torch.float16\n",
      "91,\t model.layers.10.self_attn.q_proj.weight, \t cuda:3 \t torch.int8\n",
      "92,\t model.layers.10.self_attn.k_proj.weight, \t cuda:3 \t torch.int8\n",
      "93,\t model.layers.10.self_attn.v_proj.weight, \t cuda:3 \t torch.int8\n",
      "94,\t model.layers.10.self_attn.o_proj.weight, \t cuda:3 \t torch.int8\n",
      "95,\t model.layers.10.mlp.gate_proj.weight, \t cuda:3 \t torch.int8\n",
      "96,\t model.layers.10.mlp.up_proj.weight, \t cuda:3 \t torch.int8\n",
      "97,\t model.layers.10.mlp.down_proj.weight, \t cuda:3 \t torch.int8\n",
      "98,\t model.layers.10.input_layernorm.weight, \t cuda:3 \t torch.float16\n",
      "99,\t model.layers.10.post_attention_layernorm.weight, \t cuda:3 \t torch.float16\n",
      "100,\t model.layers.11.self_attn.q_proj.weight, \t cuda:3 \t torch.int8\n",
      "101,\t model.layers.11.self_attn.k_proj.weight, \t cuda:3 \t torch.int8\n",
      "102,\t model.layers.11.self_attn.v_proj.weight, \t cuda:3 \t torch.int8\n",
      "103,\t model.layers.11.self_attn.o_proj.weight, \t cuda:3 \t torch.int8\n",
      "104,\t model.layers.11.mlp.gate_proj.weight, \t cuda:3 \t torch.int8\n",
      "105,\t model.layers.11.mlp.up_proj.weight, \t cuda:3 \t torch.int8\n",
      "106,\t model.layers.11.mlp.down_proj.weight, \t cuda:3 \t torch.int8\n",
      "107,\t model.layers.11.input_layernorm.weight, \t cuda:3 \t torch.float16\n",
      "108,\t model.layers.11.post_attention_layernorm.weight, \t cuda:3 \t torch.float16\n",
      "109,\t model.layers.12.self_attn.q_proj.weight, \t cuda:3 \t torch.int8\n",
      "110,\t model.layers.12.self_attn.k_proj.weight, \t cuda:3 \t torch.int8\n",
      "111,\t model.layers.12.self_attn.v_proj.weight, \t cuda:3 \t torch.int8\n",
      "112,\t model.layers.12.self_attn.o_proj.weight, \t cuda:3 \t torch.int8\n",
      "113,\t model.layers.12.mlp.gate_proj.weight, \t cuda:3 \t torch.int8\n",
      "114,\t model.layers.12.mlp.up_proj.weight, \t cuda:3 \t torch.int8\n",
      "115,\t model.layers.12.mlp.down_proj.weight, \t cuda:3 \t torch.int8\n",
      "116,\t model.layers.12.input_layernorm.weight, \t cuda:3 \t torch.float16\n",
      "117,\t model.layers.12.post_attention_layernorm.weight, \t cuda:3 \t torch.float16\n",
      "118,\t model.layers.13.self_attn.q_proj.weight, \t cuda:3 \t torch.int8\n",
      "119,\t model.layers.13.self_attn.k_proj.weight, \t cuda:3 \t torch.int8\n",
      "120,\t model.layers.13.self_attn.v_proj.weight, \t cuda:3 \t torch.int8\n",
      "121,\t model.layers.13.self_attn.o_proj.weight, \t cuda:3 \t torch.int8\n",
      "122,\t model.layers.13.mlp.gate_proj.weight, \t cuda:3 \t torch.int8\n",
      "123,\t model.layers.13.mlp.up_proj.weight, \t cuda:3 \t torch.int8\n",
      "124,\t model.layers.13.mlp.down_proj.weight, \t cuda:3 \t torch.int8\n",
      "125,\t model.layers.13.input_layernorm.weight, \t cuda:3 \t torch.float16\n",
      "126,\t model.layers.13.post_attention_layernorm.weight, \t cuda:3 \t torch.float16\n",
      "127,\t model.layers.14.self_attn.q_proj.weight, \t cuda:4 \t torch.int8\n",
      "128,\t model.layers.14.self_attn.k_proj.weight, \t cuda:4 \t torch.int8\n",
      "129,\t model.layers.14.self_attn.v_proj.weight, \t cuda:4 \t torch.int8\n",
      "130,\t model.layers.14.self_attn.o_proj.weight, \t cuda:4 \t torch.int8\n",
      "131,\t model.layers.14.mlp.gate_proj.weight, \t cuda:4 \t torch.int8\n",
      "132,\t model.layers.14.mlp.up_proj.weight, \t cuda:4 \t torch.int8\n",
      "133,\t model.layers.14.mlp.down_proj.weight, \t cuda:4 \t torch.int8\n",
      "134,\t model.layers.14.input_layernorm.weight, \t cuda:4 \t torch.float16\n",
      "135,\t model.layers.14.post_attention_layernorm.weight, \t cuda:4 \t torch.float16\n",
      "136,\t model.layers.15.self_attn.q_proj.weight, \t cuda:4 \t torch.int8\n",
      "137,\t model.layers.15.self_attn.k_proj.weight, \t cuda:4 \t torch.int8\n",
      "138,\t model.layers.15.self_attn.v_proj.weight, \t cuda:4 \t torch.int8\n",
      "139,\t model.layers.15.self_attn.o_proj.weight, \t cuda:4 \t torch.int8\n",
      "140,\t model.layers.15.mlp.gate_proj.weight, \t cuda:4 \t torch.int8\n",
      "141,\t model.layers.15.mlp.up_proj.weight, \t cuda:4 \t torch.int8\n",
      "142,\t model.layers.15.mlp.down_proj.weight, \t cuda:4 \t torch.int8\n",
      "143,\t model.layers.15.input_layernorm.weight, \t cuda:4 \t torch.float16\n",
      "144,\t model.layers.15.post_attention_layernorm.weight, \t cuda:4 \t torch.float16\n",
      "145,\t model.layers.16.self_attn.q_proj.weight, \t cuda:4 \t torch.int8\n",
      "146,\t model.layers.16.self_attn.k_proj.weight, \t cuda:4 \t torch.int8\n",
      "147,\t model.layers.16.self_attn.v_proj.weight, \t cuda:4 \t torch.int8\n",
      "148,\t model.layers.16.self_attn.o_proj.weight, \t cuda:4 \t torch.int8\n",
      "149,\t model.layers.16.mlp.gate_proj.weight, \t cuda:4 \t torch.int8\n",
      "150,\t model.layers.16.mlp.up_proj.weight, \t cuda:4 \t torch.int8\n",
      "151,\t model.layers.16.mlp.down_proj.weight, \t cuda:4 \t torch.int8\n",
      "152,\t model.layers.16.input_layernorm.weight, \t cuda:4 \t torch.float16\n",
      "153,\t model.layers.16.post_attention_layernorm.weight, \t cuda:4 \t torch.float16\n",
      "154,\t model.layers.17.self_attn.q_proj.weight, \t cuda:4 \t torch.int8\n",
      "155,\t model.layers.17.self_attn.k_proj.weight, \t cuda:4 \t torch.int8\n",
      "156,\t model.layers.17.self_attn.v_proj.weight, \t cuda:4 \t torch.int8\n",
      "157,\t model.layers.17.self_attn.o_proj.weight, \t cuda:4 \t torch.int8\n",
      "158,\t model.layers.17.mlp.gate_proj.weight, \t cuda:4 \t torch.int8\n",
      "159,\t model.layers.17.mlp.up_proj.weight, \t cuda:4 \t torch.int8\n",
      "160,\t model.layers.17.mlp.down_proj.weight, \t cuda:4 \t torch.int8\n",
      "161,\t model.layers.17.input_layernorm.weight, \t cuda:4 \t torch.float16\n",
      "162,\t model.layers.17.post_attention_layernorm.weight, \t cuda:4 \t torch.float16\n",
      "163,\t model.layers.18.self_attn.q_proj.weight, \t cuda:5 \t torch.int8\n",
      "164,\t model.layers.18.self_attn.k_proj.weight, \t cuda:5 \t torch.int8\n",
      "165,\t model.layers.18.self_attn.v_proj.weight, \t cuda:5 \t torch.int8\n",
      "166,\t model.layers.18.self_attn.o_proj.weight, \t cuda:5 \t torch.int8\n",
      "167,\t model.layers.18.mlp.gate_proj.weight, \t cuda:5 \t torch.int8\n",
      "168,\t model.layers.18.mlp.up_proj.weight, \t cuda:5 \t torch.int8\n",
      "169,\t model.layers.18.mlp.down_proj.weight, \t cuda:5 \t torch.int8\n",
      "170,\t model.layers.18.input_layernorm.weight, \t cuda:5 \t torch.float16\n",
      "171,\t model.layers.18.post_attention_layernorm.weight, \t cuda:5 \t torch.float16\n",
      "172,\t model.layers.19.self_attn.q_proj.weight, \t cuda:5 \t torch.int8\n",
      "173,\t model.layers.19.self_attn.k_proj.weight, \t cuda:5 \t torch.int8\n",
      "174,\t model.layers.19.self_attn.v_proj.weight, \t cuda:5 \t torch.int8\n",
      "175,\t model.layers.19.self_attn.o_proj.weight, \t cuda:5 \t torch.int8\n",
      "176,\t model.layers.19.mlp.gate_proj.weight, \t cuda:5 \t torch.int8\n",
      "177,\t model.layers.19.mlp.up_proj.weight, \t cuda:5 \t torch.int8\n",
      "178,\t model.layers.19.mlp.down_proj.weight, \t cuda:5 \t torch.int8\n",
      "179,\t model.layers.19.input_layernorm.weight, \t cuda:5 \t torch.float16\n",
      "180,\t model.layers.19.post_attention_layernorm.weight, \t cuda:5 \t torch.float16\n",
      "181,\t model.layers.20.self_attn.q_proj.weight, \t cuda:5 \t torch.int8\n",
      "182,\t model.layers.20.self_attn.k_proj.weight, \t cuda:5 \t torch.int8\n",
      "183,\t model.layers.20.self_attn.v_proj.weight, \t cuda:5 \t torch.int8\n",
      "184,\t model.layers.20.self_attn.o_proj.weight, \t cuda:5 \t torch.int8\n",
      "185,\t model.layers.20.mlp.gate_proj.weight, \t cuda:5 \t torch.int8\n",
      "186,\t model.layers.20.mlp.up_proj.weight, \t cuda:5 \t torch.int8\n",
      "187,\t model.layers.20.mlp.down_proj.weight, \t cuda:5 \t torch.int8\n",
      "188,\t model.layers.20.input_layernorm.weight, \t cuda:5 \t torch.float16\n",
      "189,\t model.layers.20.post_attention_layernorm.weight, \t cuda:5 \t torch.float16\n",
      "190,\t model.layers.21.self_attn.q_proj.weight, \t cuda:5 \t torch.int8\n",
      "191,\t model.layers.21.self_attn.k_proj.weight, \t cuda:5 \t torch.int8\n",
      "192,\t model.layers.21.self_attn.v_proj.weight, \t cuda:5 \t torch.int8\n",
      "193,\t model.layers.21.self_attn.o_proj.weight, \t cuda:5 \t torch.int8\n",
      "194,\t model.layers.21.mlp.gate_proj.weight, \t cuda:5 \t torch.int8\n",
      "195,\t model.layers.21.mlp.up_proj.weight, \t cuda:5 \t torch.int8\n",
      "196,\t model.layers.21.mlp.down_proj.weight, \t cuda:5 \t torch.int8\n",
      "197,\t model.layers.21.input_layernorm.weight, \t cuda:5 \t torch.float16\n",
      "198,\t model.layers.21.post_attention_layernorm.weight, \t cuda:5 \t torch.float16\n",
      "199,\t model.layers.22.self_attn.q_proj.weight, \t cuda:6 \t torch.int8\n",
      "200,\t model.layers.22.self_attn.k_proj.weight, \t cuda:6 \t torch.int8\n",
      "201,\t model.layers.22.self_attn.v_proj.weight, \t cuda:6 \t torch.int8\n",
      "202,\t model.layers.22.self_attn.o_proj.weight, \t cuda:6 \t torch.int8\n",
      "203,\t model.layers.22.mlp.gate_proj.weight, \t cuda:6 \t torch.int8\n",
      "204,\t model.layers.22.mlp.up_proj.weight, \t cuda:6 \t torch.int8\n",
      "205,\t model.layers.22.mlp.down_proj.weight, \t cuda:6 \t torch.int8\n",
      "206,\t model.layers.22.input_layernorm.weight, \t cuda:6 \t torch.float16\n",
      "207,\t model.layers.22.post_attention_layernorm.weight, \t cuda:6 \t torch.float16\n",
      "208,\t model.layers.23.self_attn.q_proj.weight, \t cuda:6 \t torch.int8\n",
      "209,\t model.layers.23.self_attn.k_proj.weight, \t cuda:6 \t torch.int8\n",
      "210,\t model.layers.23.self_attn.v_proj.weight, \t cuda:6 \t torch.int8\n",
      "211,\t model.layers.23.self_attn.o_proj.weight, \t cuda:6 \t torch.int8\n",
      "212,\t model.layers.23.mlp.gate_proj.weight, \t cuda:6 \t torch.int8\n",
      "213,\t model.layers.23.mlp.up_proj.weight, \t cuda:6 \t torch.int8\n",
      "214,\t model.layers.23.mlp.down_proj.weight, \t cuda:6 \t torch.int8\n",
      "215,\t model.layers.23.input_layernorm.weight, \t cuda:6 \t torch.float16\n",
      "216,\t model.layers.23.post_attention_layernorm.weight, \t cuda:6 \t torch.float16\n",
      "217,\t model.layers.24.self_attn.q_proj.weight, \t cuda:6 \t torch.int8\n",
      "218,\t model.layers.24.self_attn.k_proj.weight, \t cuda:6 \t torch.int8\n",
      "219,\t model.layers.24.self_attn.v_proj.weight, \t cuda:6 \t torch.int8\n",
      "220,\t model.layers.24.self_attn.o_proj.weight, \t cuda:6 \t torch.int8\n",
      "221,\t model.layers.24.mlp.gate_proj.weight, \t cuda:6 \t torch.int8\n",
      "222,\t model.layers.24.mlp.up_proj.weight, \t cuda:6 \t torch.int8\n",
      "223,\t model.layers.24.mlp.down_proj.weight, \t cuda:6 \t torch.int8\n",
      "224,\t model.layers.24.input_layernorm.weight, \t cuda:6 \t torch.float16\n",
      "225,\t model.layers.24.post_attention_layernorm.weight, \t cuda:6 \t torch.float16\n",
      "226,\t model.layers.25.self_attn.q_proj.weight, \t cuda:6 \t torch.int8\n",
      "227,\t model.layers.25.self_attn.k_proj.weight, \t cuda:6 \t torch.int8\n",
      "228,\t model.layers.25.self_attn.v_proj.weight, \t cuda:6 \t torch.int8\n",
      "229,\t model.layers.25.self_attn.o_proj.weight, \t cuda:6 \t torch.int8\n",
      "230,\t model.layers.25.mlp.gate_proj.weight, \t cuda:6 \t torch.int8\n",
      "231,\t model.layers.25.mlp.up_proj.weight, \t cuda:6 \t torch.int8\n",
      "232,\t model.layers.25.mlp.down_proj.weight, \t cuda:6 \t torch.int8\n",
      "233,\t model.layers.25.input_layernorm.weight, \t cuda:6 \t torch.float16\n",
      "234,\t model.layers.25.post_attention_layernorm.weight, \t cuda:6 \t torch.float16\n",
      "235,\t model.layers.26.self_attn.q_proj.weight, \t cuda:7 \t torch.int8\n",
      "236,\t model.layers.26.self_attn.k_proj.weight, \t cuda:7 \t torch.int8\n",
      "237,\t model.layers.26.self_attn.v_proj.weight, \t cuda:7 \t torch.int8\n",
      "238,\t model.layers.26.self_attn.o_proj.weight, \t cuda:7 \t torch.int8\n",
      "239,\t model.layers.26.mlp.gate_proj.weight, \t cuda:7 \t torch.int8\n",
      "240,\t model.layers.26.mlp.up_proj.weight, \t cuda:7 \t torch.int8\n",
      "241,\t model.layers.26.mlp.down_proj.weight, \t cuda:7 \t torch.int8\n",
      "242,\t model.layers.26.input_layernorm.weight, \t cuda:7 \t torch.float16\n",
      "243,\t model.layers.26.post_attention_layernorm.weight, \t cuda:7 \t torch.float16\n",
      "244,\t model.layers.27.self_attn.q_proj.weight, \t cuda:7 \t torch.int8\n",
      "245,\t model.layers.27.self_attn.k_proj.weight, \t cuda:7 \t torch.int8\n",
      "246,\t model.layers.27.self_attn.v_proj.weight, \t cuda:7 \t torch.int8\n",
      "247,\t model.layers.27.self_attn.o_proj.weight, \t cuda:7 \t torch.int8\n",
      "248,\t model.layers.27.mlp.gate_proj.weight, \t cuda:7 \t torch.int8\n",
      "249,\t model.layers.27.mlp.up_proj.weight, \t cuda:7 \t torch.int8\n",
      "250,\t model.layers.27.mlp.down_proj.weight, \t cuda:7 \t torch.int8\n",
      "251,\t model.layers.27.input_layernorm.weight, \t cuda:7 \t torch.float16\n",
      "252,\t model.layers.27.post_attention_layernorm.weight, \t cuda:7 \t torch.float16\n",
      "253,\t model.layers.28.self_attn.q_proj.weight, \t cuda:7 \t torch.int8\n",
      "254,\t model.layers.28.self_attn.k_proj.weight, \t cuda:7 \t torch.int8\n",
      "255,\t model.layers.28.self_attn.v_proj.weight, \t cuda:7 \t torch.int8\n",
      "256,\t model.layers.28.self_attn.o_proj.weight, \t cuda:7 \t torch.int8\n",
      "257,\t model.layers.28.mlp.gate_proj.weight, \t cuda:7 \t torch.int8\n",
      "258,\t model.layers.28.mlp.up_proj.weight, \t cuda:7 \t torch.int8\n",
      "259,\t model.layers.28.mlp.down_proj.weight, \t cuda:7 \t torch.int8\n",
      "260,\t model.layers.28.input_layernorm.weight, \t cuda:7 \t torch.float16\n",
      "261,\t model.layers.28.post_attention_layernorm.weight, \t cuda:7 \t torch.float16\n",
      "262,\t model.layers.29.self_attn.q_proj.weight, \t cuda:7 \t torch.int8\n",
      "263,\t model.layers.29.self_attn.k_proj.weight, \t cuda:7 \t torch.int8\n",
      "264,\t model.layers.29.self_attn.v_proj.weight, \t cuda:7 \t torch.int8\n",
      "265,\t model.layers.29.self_attn.o_proj.weight, \t cuda:7 \t torch.int8\n",
      "266,\t model.layers.29.mlp.gate_proj.weight, \t cuda:7 \t torch.int8\n",
      "267,\t model.layers.29.mlp.up_proj.weight, \t cuda:7 \t torch.int8\n",
      "268,\t model.layers.29.mlp.down_proj.weight, \t cuda:7 \t torch.int8\n",
      "269,\t model.layers.29.input_layernorm.weight, \t cuda:7 \t torch.float16\n",
      "270,\t model.layers.29.post_attention_layernorm.weight, \t cuda:7 \t torch.float16\n",
      "271,\t model.layers.30.self_attn.q_proj.weight, \t cuda:8 \t torch.int8\n",
      "272,\t model.layers.30.self_attn.k_proj.weight, \t cuda:8 \t torch.int8\n",
      "273,\t model.layers.30.self_attn.v_proj.weight, \t cuda:8 \t torch.int8\n",
      "274,\t model.layers.30.self_attn.o_proj.weight, \t cuda:8 \t torch.int8\n",
      "275,\t model.layers.30.mlp.gate_proj.weight, \t cuda:8 \t torch.int8\n",
      "276,\t model.layers.30.mlp.up_proj.weight, \t cuda:8 \t torch.int8\n",
      "277,\t model.layers.30.mlp.down_proj.weight, \t cuda:8 \t torch.int8\n",
      "278,\t model.layers.30.input_layernorm.weight, \t cuda:8 \t torch.float16\n",
      "279,\t model.layers.30.post_attention_layernorm.weight, \t cuda:8 \t torch.float16\n",
      "280,\t model.layers.31.self_attn.q_proj.weight, \t cuda:8 \t torch.int8\n",
      "281,\t model.layers.31.self_attn.k_proj.weight, \t cuda:8 \t torch.int8\n",
      "282,\t model.layers.31.self_attn.v_proj.weight, \t cuda:8 \t torch.int8\n",
      "283,\t model.layers.31.self_attn.o_proj.weight, \t cuda:8 \t torch.int8\n",
      "284,\t model.layers.31.mlp.gate_proj.weight, \t cuda:8 \t torch.int8\n",
      "285,\t model.layers.31.mlp.up_proj.weight, \t cuda:8 \t torch.int8\n",
      "286,\t model.layers.31.mlp.down_proj.weight, \t cuda:8 \t torch.int8\n",
      "287,\t model.layers.31.input_layernorm.weight, \t cuda:8 \t torch.float16\n",
      "288,\t model.layers.31.post_attention_layernorm.weight, \t cuda:8 \t torch.float16\n",
      "289,\t model.norm.weight, \t cuda:8 \t torch.float16\n",
      "290,\t lm_head.weight, \t cuda:8 \t torch.float16\n"
     ]
    }
   ],
   "source": [
    "for i, para in enumerate(model.named_parameters()):\n",
    "    print(f\"{i},\\t {para[0]}, \\t {para[1].device} \\t {para[1].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e930c97",
   "metadata": {},
   "source": [
    "toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e51b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.net1 = torch.nn.Linear(10000, 10).to('cuda:0')\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.net2 = torch.nn.Linear(10,5).to('cuda:1')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.net1(x.to('cuda:0')))\n",
    "        return self.net2(x.to('cuda:1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d089fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcec929",
   "metadata": {},
   "source": [
    "PyTorch 的全连接层（`Linear` 层）数学形式是：`y = xW^T + b`\n",
    "\n",
    "其中：\n",
    "- 输入维度 `in_features = 10000`\n",
    "- 输出维度 `out_features = 10`\n",
    "\n",
    "---\n",
    "\n",
    "### 参数分析\n",
    "\n",
    "`torch.nn.Linear` 层包含两个参数：\n",
    "1. **权重（weight）**：形状为 `(out_features, in_features)` → `(10, 10000)`\n",
    "2. **偏置（bias）**：形状为 `(out_features,)` → `(10,)`\n",
    "\n",
    "---\n",
    "\n",
    "### `model.net1.parameters()` 是什么？\n",
    "\n",
    "`model.net1.parameters()` 是一个 **Python 生成器**，它会依次返回该层的所有可训练参数（即 weight 和 bias）。\n",
    "\n",
    "### 补充说明\n",
    "\n",
    "- `parameters()` 返回的是 **可训练参数**，即会被优化器更新的张量。\n",
    "- 如果你只想看某个参数，比如权重，可以直接访问：\n",
    "  ```python\n",
    "  model.net1.weight.shape  # torch.Size([10, 10000])\n",
    "  model.net1.bias.shape    # torch.Size([10])\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 小结一句话：\n",
    "\n",
    "> `model.net1.parameters()` 会返回两个张量：一个是形状为 `(10, 10000)` 的权重矩阵，另一个是形状为 `(10,)` 的偏置向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a3f586b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "print(next(model.net1.parameters()).device)\n",
    "print(list(model.net2.parameters())[0].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91eb19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToyModel()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "outputs = model(torch.randn(20, 10000))\n",
    "labels = torch.randn(20, 5).to('cuda:1') \n",
    "loss_fn(outputs, labels).backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b48c81",
   "metadata": {},
   "source": [
    "split ResNet\n",
    "\n",
    "model = ResNet(block, layers, **kwargs)\n",
    "\n",
    "**resnet18**\n",
    "_resnet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "**resnet34**\n",
    "_resnet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "**resnet50**\n",
    "_resnet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "**resnet101**\n",
    "_resnet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "**resnet152**\n",
    "_resnet(Bottleneck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe4e50da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models.resnet import ResNet, Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99b52bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(Bottleneck, [3,4,6,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd109f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ca3b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e13cf3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 64, 64]             128\n",
      "              ReLU-3           [-1, 64, 64, 64]               0\n",
      "         MaxPool2d-4           [-1, 64, 32, 32]               0\n",
      "            Conv2d-5           [-1, 64, 32, 32]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
      "              ReLU-7           [-1, 64, 32, 32]               0\n",
      "            Conv2d-8           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
      "             ReLU-10           [-1, 64, 32, 32]               0\n",
      "           Conv2d-11          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 32, 32]             512\n",
      "           Conv2d-13          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 32, 32]             512\n",
      "             ReLU-15          [-1, 256, 32, 32]               0\n",
      "       Bottleneck-16          [-1, 256, 32, 32]               0\n",
      "           Conv2d-17           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 32, 32]             128\n",
      "             ReLU-19           [-1, 64, 32, 32]               0\n",
      "           Conv2d-20           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 32, 32]             128\n",
      "             ReLU-22           [-1, 64, 32, 32]               0\n",
      "           Conv2d-23          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 32, 32]             512\n",
      "             ReLU-25          [-1, 256, 32, 32]               0\n",
      "       Bottleneck-26          [-1, 256, 32, 32]               0\n",
      "           Conv2d-27           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 32, 32]             128\n",
      "             ReLU-29           [-1, 64, 32, 32]               0\n",
      "           Conv2d-30           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 32, 32]             128\n",
      "             ReLU-32           [-1, 64, 32, 32]               0\n",
      "           Conv2d-33          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 32, 32]             512\n",
      "             ReLU-35          [-1, 256, 32, 32]               0\n",
      "       Bottleneck-36          [-1, 256, 32, 32]               0\n",
      "           Conv2d-37          [-1, 128, 32, 32]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 32, 32]             256\n",
      "             ReLU-39          [-1, 128, 32, 32]               0\n",
      "           Conv2d-40          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 16, 16]             256\n",
      "             ReLU-42          [-1, 128, 16, 16]               0\n",
      "           Conv2d-43          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 16, 16]           1,024\n",
      "           Conv2d-45          [-1, 512, 16, 16]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-47          [-1, 512, 16, 16]               0\n",
      "       Bottleneck-48          [-1, 512, 16, 16]               0\n",
      "           Conv2d-49          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 16, 16]             256\n",
      "             ReLU-51          [-1, 128, 16, 16]               0\n",
      "           Conv2d-52          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 16, 16]             256\n",
      "             ReLU-54          [-1, 128, 16, 16]               0\n",
      "           Conv2d-55          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-57          [-1, 512, 16, 16]               0\n",
      "       Bottleneck-58          [-1, 512, 16, 16]               0\n",
      "           Conv2d-59          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 16, 16]             256\n",
      "             ReLU-61          [-1, 128, 16, 16]               0\n",
      "           Conv2d-62          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 16, 16]             256\n",
      "             ReLU-64          [-1, 128, 16, 16]               0\n",
      "           Conv2d-65          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-67          [-1, 512, 16, 16]               0\n",
      "       Bottleneck-68          [-1, 512, 16, 16]               0\n",
      "           Conv2d-69          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 16, 16]             256\n",
      "             ReLU-71          [-1, 128, 16, 16]               0\n",
      "           Conv2d-72          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 16, 16]             256\n",
      "             ReLU-74          [-1, 128, 16, 16]               0\n",
      "           Conv2d-75          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-77          [-1, 512, 16, 16]               0\n",
      "       Bottleneck-78          [-1, 512, 16, 16]               0\n",
      "           Conv2d-79          [-1, 256, 16, 16]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 16, 16]             512\n",
      "             ReLU-81          [-1, 256, 16, 16]               0\n",
      "           Conv2d-82            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-83            [-1, 256, 8, 8]             512\n",
      "             ReLU-84            [-1, 256, 8, 8]               0\n",
      "           Conv2d-85           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-86           [-1, 1024, 8, 8]           2,048\n",
      "           Conv2d-87           [-1, 1024, 8, 8]         524,288\n",
      "      BatchNorm2d-88           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-89           [-1, 1024, 8, 8]               0\n",
      "       Bottleneck-90           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-91            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-92            [-1, 256, 8, 8]             512\n",
      "             ReLU-93            [-1, 256, 8, 8]               0\n",
      "           Conv2d-94            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-95            [-1, 256, 8, 8]             512\n",
      "             ReLU-96            [-1, 256, 8, 8]               0\n",
      "           Conv2d-97           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-98           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-99           [-1, 1024, 8, 8]               0\n",
      "      Bottleneck-100           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-101            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-102            [-1, 256, 8, 8]             512\n",
      "            ReLU-103            [-1, 256, 8, 8]               0\n",
      "          Conv2d-104            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-105            [-1, 256, 8, 8]             512\n",
      "            ReLU-106            [-1, 256, 8, 8]               0\n",
      "          Conv2d-107           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-108           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-109           [-1, 1024, 8, 8]               0\n",
      "      Bottleneck-110           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-111            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-112            [-1, 256, 8, 8]             512\n",
      "            ReLU-113            [-1, 256, 8, 8]               0\n",
      "          Conv2d-114            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-115            [-1, 256, 8, 8]             512\n",
      "            ReLU-116            [-1, 256, 8, 8]               0\n",
      "          Conv2d-117           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-118           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-119           [-1, 1024, 8, 8]               0\n",
      "      Bottleneck-120           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-121            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-122            [-1, 256, 8, 8]             512\n",
      "            ReLU-123            [-1, 256, 8, 8]               0\n",
      "          Conv2d-124            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-125            [-1, 256, 8, 8]             512\n",
      "            ReLU-126            [-1, 256, 8, 8]               0\n",
      "          Conv2d-127           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-128           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-129           [-1, 1024, 8, 8]               0\n",
      "      Bottleneck-130           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-131            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-132            [-1, 256, 8, 8]             512\n",
      "            ReLU-133            [-1, 256, 8, 8]               0\n",
      "          Conv2d-134            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-135            [-1, 256, 8, 8]             512\n",
      "            ReLU-136            [-1, 256, 8, 8]               0\n",
      "          Conv2d-137           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-138           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-139           [-1, 1024, 8, 8]               0\n",
      "      Bottleneck-140           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-141            [-1, 512, 8, 8]         524,288\n",
      "     BatchNorm2d-142            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-143            [-1, 512, 8, 8]               0\n",
      "          Conv2d-144            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-146            [-1, 512, 4, 4]               0\n",
      "          Conv2d-147           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 4, 4]           4,096\n",
      "          Conv2d-149           [-1, 2048, 4, 4]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 4, 4]           4,096\n",
      "            ReLU-151           [-1, 2048, 4, 4]               0\n",
      "      Bottleneck-152           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-153            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-155            [-1, 512, 4, 4]               0\n",
      "          Conv2d-156            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-158            [-1, 512, 4, 4]               0\n",
      "          Conv2d-159           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 4, 4]           4,096\n",
      "            ReLU-161           [-1, 2048, 4, 4]               0\n",
      "      Bottleneck-162           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-163            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-165            [-1, 512, 4, 4]               0\n",
      "          Conv2d-166            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-168            [-1, 512, 4, 4]               0\n",
      "          Conv2d-169           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 4, 4]           4,096\n",
      "            ReLU-171           [-1, 2048, 4, 4]               0\n",
      "      Bottleneck-172           [-1, 2048, 4, 4]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 25,557,032\n",
      "Trainable params: 25,557,032\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 93.59\n",
      "Params size (MB): 97.49\n",
      "Estimated Total Size (MB): 191.27\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(3, 128, 128), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a5b387",
   "metadata": {},
   "source": [
    "自定义模型并行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac8f8638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.rand((2,3,4))\n",
    "print(t.shape)\n",
    "t.view(t.size(0),-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a88a4801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParallerResNet50(ResNet):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__(Bottleneck, [3, 4, 6, 3], num_classes=num_classes)\n",
    "        self.seq1 = nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.bn1,\n",
    "            self.relu,\n",
    "            self.maxpool,\n",
    "            self.layer1, \n",
    "            self.layer2\n",
    "        ).to('cuda:0')\n",
    "\n",
    "        self.seq2 = nn.Sequential(\n",
    "            self.layer3,\n",
    "            self.layer4,\n",
    "            self.avgpool,\n",
    "        ).to('cuda:5')\n",
    "\n",
    "        self.fc.to('cuda:7')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.seq2(self.seq1(x.to('cuda:0')).to('cuda:5'))\n",
    "        return self.fc(x.view(x.size(0),-1).to('cuda:7'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "60c76d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_size(model):\n",
    "    return sum([para.numel() for para in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a79af5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25557032"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_size(ResNet(Bottleneck, [3, 4, 6, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c15a52b",
   "metadata": {},
   "source": [
    "train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "da05f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1050e5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [8],\n",
       "        [8],\n",
       "        [2],\n",
       "        [2]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_indices = torch.LongTensor(5) \\\n",
    "                           .random_(0, num_classes) \\\n",
    "                           .view(5, 1)\n",
    "one_hot_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "83e385aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.zeros(5, num_classes).scatter_(1, one_hot_indices, 1)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "22d41ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1]*10).unsqueeze(0)\n",
    "print(a)\n",
    "torch.zeros(5, num_classes).scatter_(0, a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5956b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 1000\n",
    "num_batches = 3\n",
    "batch_size = 120\n",
    "image_w = 128\n",
    "image_h = 128\n",
    "\n",
    "def train(model):\n",
    "    model.train(True)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "    one_hot_indices = torch.LongTensor(batch_size) \\\n",
    "                           .random_(0, num_classes) \\\n",
    "                           .view(batch_size, 1)\n",
    "    \n",
    "    for _ in range(num_batches):\n",
    "        inputs = torch.randn(batch_size, 3, image_w, image_h)\n",
    "        labels = torch.zeros(batch_size, num_classes).scatter_(1, one_hot_indices, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.to('cuda:0'))\n",
    "        print(\"outputs.device:\",outputs.device)\n",
    "        labels = labels.to(outputs.device)\n",
    "        loss_fn(outputs, labels).backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c7d3f41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:7\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n",
      "outputs.device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.switch_backend('Agg')\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "num_repeat = 10\n",
    "\n",
    "stmt = \"train(model)\"\n",
    "\n",
    "# 模型并行\n",
    "setup = \"model = ModelParallerResNet50()\"\n",
    "mp_run_times = timeit.repeat(\n",
    "    stmt, setup, number=1, repeat=num_repeat, globals=globals())\n",
    "mp_mean, mp_std = np.mean(mp_run_times), np.std(mp_run_times)\n",
    "\n",
    "# 单卡\n",
    "setup = \"import torchvision.models as models;\" + \\\n",
    "        \"model = models.resnet50(num_classes=num_classes).to('cuda:0')\"\n",
    "rn_run_times = timeit.repeat(\n",
    "    stmt, setup, number=1, repeat=num_repeat, globals=globals())\n",
    "rn_mean, rn_std = np.mean(rn_run_times), np.std(rn_run_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ac44b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(means, stds, labels, fig_name):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(np.arange(len(means)), means, yerr=stds,\n",
    "           align='center', alpha=0.5, ecolor='red', capsize=10, width=0.6)\n",
    "    ax.set_ylabel('ResNet50 Execution Time (Second)')\n",
    "    ax.set_xticks(np.arange(len(means)))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.yaxis.grid(True)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d0eff561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot([mp_mean, rn_mean],\n",
    "     [mp_std, rn_std],\n",
    "     ['Model Parallel', 'Single GPU'],\n",
    "     'mp_vs_rn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "26bd76bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4490540227969177, 0.37082438777433707)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp_mean, rn_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c721dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTroch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
