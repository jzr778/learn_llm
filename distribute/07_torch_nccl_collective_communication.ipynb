{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234b8e4a",
   "metadata": {},
   "source": [
    "- `export NCCL_P2P_DISABLE=1`:disable direct GPU-to-GPU(P2P) communincation\n",
    "    - 禁用点对点通信，点对点通信是一种高效的数据传输方式，允许直接在 GPU 之间传输数据，绕过主机内存（CPU 内存）。但在某些情况下，点对点通信可能会导致兼容性或性能问题\n",
    "    - 在 RTX 40 系显卡上，NCCL 默认的 GPU 直连（P2P）通信可能不被支持或会出错，因此主动关掉它，让数据走传统路径（经 CPU 内存），避免崩溃或性能反降"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e6c56d",
   "metadata": {},
   "source": [
    "\n",
    "``` python\n",
    "\"\"\"pytorch分布式相关api\"\"\"\n",
    "import torch.distributed as dist\n",
    "dist.init_process_group() #初始化进程组\n",
    "dist.get_rank() #获得当前进程rank\n",
    "dist.get_world_size() #获取进程组的进程总数\n",
    "dist.barrier() #同步进程组内的所有进程，阻塞所有进程直到所有进程都执行到操作\n",
    "```\n",
    "\n",
    "- `p.join()` 是 Python 进程级同步——“主进程等子进程 生命结束”。\n",
    "- `torch.distributed.barrier()` 是 分布式集合通信——“所有 逻辑 rank 互相等，直到 全部到达才继续往下执行”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67662a6a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### scatter\n",
    "\n",
    "``` python\n",
    "def dist_scatter():\n",
    "    dist.barrier()\n",
    "\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "\n",
    "    tensor = torch.zeros(world_size)\n",
    "    before_tensor = tensor.clone()\n",
    "    \n",
    "    if dist.get_rank() == 0:\n",
    "        t_ones = torch.ones(world_size)\n",
    "        t_fives = torch.ones(world_size) * 5\n",
    "        scatter_list = [t_ones, t_fives]\n",
    "    else:\n",
    "        scatter_list = None\n",
    "    dist.scatter(tensor, scatter_list, src)\n",
    "    logging.info(f\"scatter, rank: {rank}, before scatter: {repr(before_tensor)} after scatter: {repr(tensor)}\")\n",
    "    dist.barrier()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbd607e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### gather\n",
    "\n",
    "``` python\n",
    "\n",
    "def dist_gather():\n",
    "    dist.barrier()\n",
    "\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "\n",
    "    tensor = torch.tensor([rank*2+1], dtype=torch.float32)\n",
    "    before_tensor = tensor.clone()\n",
    "\n",
    "    gather_list = [torch.zeros(1) for _ in range(world_size)] if rank == 0 else None\n",
    "\n",
    "    dist.gather(tensor, gather_list, dst=0)\n",
    "    \n",
    "    logging.info(f\"gather, rank: {rank}, before gather: {repr(before_tensor)} after gather: {repr(gather_list)}\")\n",
    "    dist.barrier()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb179c4d",
   "metadata": {},
   "source": [
    "---\n",
    "### broadcast\n",
    "\n",
    "``` python\n",
    "def dist_broadcast():\n",
    "    dist.barrier()\n",
    "\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "\n",
    "    src_rank = 0\n",
    "    tensor = torch.tensor(world_size) if rank == src_rank else torch.zeros(1, dtype=torch.int64)\n",
    "    before_tensor = tensor.clone()\n",
    "    dist.broadcast(tensor, src=src_rank)\n",
    "    logging.info(f\"broadcast, rank: {rank}, before broadcast tensor: {repr(before_tensor)} after broadcast tensor: {repr(tensor)}\")\n",
    "    dist.barrier()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5faa95a",
   "metadata": {},
   "source": [
    "---\n",
    "### reduce\n",
    "\n",
    "``` python\n",
    "def dist_reduce():\n",
    "    dist.barrier()\n",
    "\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "\n",
    "    tensor = torch.tensor([rank*2 + 1], dtype=torch.float32)\n",
    "    before_tensor = tensor.clone()\n",
    "\n",
    "    dist.reduce(tensor, op=ReduceOp.SUM, dst=0)\n",
    "    \n",
    "    logging.info(f\"reduce, rank: {rank}, before reduce: {repr(before_tensor)} after reduce: {repr(tensor)}\")\n",
    "    dist.barrier()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daeed37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### all-reduce\n",
    "\n",
    "``` python\n",
    "def dist_allreduce():\n",
    "    dist.barrier()\n",
    "\n",
    "    rank = dist.get_rank()\n",
    "    if rank == 0:\n",
    "        tensor = torch.tensor([1., 2.])\n",
    "    else:\n",
    "        tensor = torch.tensor([2., 3.])\n",
    "    input_tensor = tensor.clone()\n",
    "    dist.all_reduce(tensor)\n",
    "\n",
    "    logging.info(f\"all_reduce, rank: {rank}, before allreduce tensor: {repr(input_tensor)}, after allreduce tensor: {repr(tensor)}\")\n",
    "    dist.barrier()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc108ad9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### all gather\n",
    "\n",
    "- gather + broadcast\n",
    "\n",
    "``` python\n",
    "def dist_allgather():\n",
    "    dist.barrier()\n",
    "\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "\n",
    "    input_tensor = torch.tensor(rank)\n",
    "    tensor_list = [torch.zeros(1, dtype=torch.int64) for _ in range(world_size)]\n",
    "    dist.all_gather(tensor_list, input_tensor)\n",
    "    logging.info(f\"allgather, rank: {rank}, input_tensor: {repr(input_tensor)}, output tensor_list: {tensor_list}\")\n",
    "    dist.barrier()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181f4a47",
   "metadata": {},
   "source": [
    "reduce + scatter\n",
    "\n",
    "``` python\n",
    "def dist_reducescatter():\n",
    "    dist.barrier()\n",
    "\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "\n",
    "    output = torch.empty(1, dtype=torch.int64)\n",
    "    input_list = [torch.tensor(rank*2+1), torch.tensor(rank*2+2)]\n",
    "    dist.reduce_scatter(output, input_list, op=ReduceOp.SUM)\n",
    "    logging.info(f\"reduce_scatter, rank: {rank}, input_list: {input_list}, tensor: {repr(output)}\")\n",
    "    dist.barrier()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b11f3c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d6105b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.]), tensor(1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1),torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2628de5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTroch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
