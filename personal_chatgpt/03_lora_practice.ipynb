{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8973ab34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiangzirou/data/miniconda3/envs/PyTroch/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb \n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8708facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "291475ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.46.3\n",
      "torch       : 2.4.1\n",
      "bitsandbytes: 0.41.3\n",
      "peft        : 0.13.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17de4f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft        : 0.13.2\n",
      "torch       : 2.4.1\n",
      "loralib     : 0.1.2\n",
      "transformers: 4.46.3\n",
      "accelerate  : 1.0.1\n",
      "datasets    : 3.1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from watermark import watermark\n",
    "print(watermark(packages='peft,torch,loralib,transformers,accelerate,datasets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7369aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os#环境代理设置\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8e46f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0e9b1f987b4d9aa3a39072c0eb5945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"bigscience/bloom-7b1\", \n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-7b1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "143ed16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"bigscience/bloom-7b1\",\n",
       "  \"apply_residual_connection_post_layernorm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BloomForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"attention_softmax_in_fp32\": true,\n",
       "  \"bias_dropout_fusion\": true,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_dropout\": 0.0,\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"masked_softmax_fusion\": true,\n",
       "  \"model_type\": \"bloom\",\n",
       "  \"n_head\": 32,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 30,\n",
       "  \"offset_alibi\": 100,\n",
       "  \"pad_token_id\": 3,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"quantization_config\": {\n",
       "    \"_load_in_4bit\": true,\n",
       "    \"_load_in_8bit\": false,\n",
       "    \"bnb_4bit_compute_dtype\": \"float32\",\n",
       "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "    \"bnb_4bit_quant_type\": \"fp4\",\n",
       "    \"bnb_4bit_use_double_quant\": false,\n",
       "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "    \"llm_int8_has_fp16_weight\": false,\n",
       "    \"llm_int8_skip_modules\": null,\n",
       "    \"llm_int8_threshold\": 6.0,\n",
       "    \"load_in_4bit\": true,\n",
       "    \"load_in_8bit\": false,\n",
       "    \"quant_method\": \"bitsandbytes\"\n",
       "  },\n",
       "  \"skip_bias_add\": true,\n",
       "  \"skip_bias_add_qkv\": false,\n",
       "  \"slow_but_exact\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.46.3\",\n",
       "  \"unk_token_id\": 0,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 250880\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eb2c71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(250880, 4096)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b3d8ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomTokenizerFast(name_or_path='bigscience/bloom-7b1', vocab_size=250680, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dac191",
   "metadata": {},
   "source": [
    "freeze original weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfebe492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1528bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "934f82b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.word_embeddings.weight       torch.float16  1,027,604,480\n",
      "transformer.word_embeddings_layernorm.weight torch.float16       4,096\n",
      "transformer.word_embeddings_layernorm.bias torch.float16       4,096\n",
      "transformer.h.0.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.0.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.0.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.0.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.0.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.0.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.0.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.0.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.0.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.0.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.0.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.0.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.1.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.1.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.1.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.1.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.1.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.1.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.1.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.1.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.1.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.1.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.1.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.1.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.2.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.2.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.2.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.2.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.2.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.2.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.2.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.2.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.2.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.2.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.2.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.2.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.3.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.3.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.3.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.3.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.3.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.3.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.3.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.3.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.3.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.3.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.3.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.3.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.4.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.4.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.4.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.4.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.4.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.4.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.4.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.4.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.4.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.4.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.4.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.4.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.5.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.5.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.5.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.5.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.5.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.5.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.5.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.5.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.5.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.5.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.5.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.5.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.6.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.6.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.6.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.6.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.6.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.6.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.6.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.6.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.6.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.6.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.6.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.6.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.7.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.7.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.7.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.7.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.7.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.7.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.7.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.7.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.7.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.7.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.7.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.7.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.8.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.8.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.8.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.8.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.8.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.8.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.8.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.8.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.8.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.8.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.8.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.8.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.9.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.9.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.9.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.9.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.9.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.9.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.9.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.9.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.9.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.9.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.9.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.9.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.10.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.10.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.10.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.10.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.10.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.10.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.10.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.10.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.10.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.10.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.10.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.10.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.11.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.11.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.11.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.11.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.11.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.11.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.11.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.11.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.11.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.11.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.11.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.11.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.12.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.12.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.12.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.12.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.12.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.12.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.12.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.12.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.12.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.12.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.12.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.12.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.13.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.13.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.13.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.13.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.13.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.13.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.13.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.13.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.13.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.13.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.13.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.13.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.14.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.14.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.14.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.14.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.14.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.14.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.14.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.14.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.14.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.14.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.14.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.14.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.15.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.15.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.15.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.15.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.15.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.15.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.15.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.15.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.15.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.15.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.15.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.15.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.16.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.16.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.16.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.16.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.16.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.16.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.16.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.16.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.16.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.16.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.16.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.16.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.17.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.17.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.17.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.17.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.17.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.17.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.17.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.17.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.17.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.17.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.17.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.17.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.18.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.18.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.18.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.18.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.18.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.18.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.18.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.18.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.18.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.18.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.18.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.18.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.19.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.19.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.19.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.19.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.19.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.19.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.19.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.19.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.19.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.19.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.19.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.19.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.20.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.20.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.20.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.20.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.20.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.20.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.20.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.20.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.20.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.20.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.20.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.20.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.21.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.21.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.21.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.21.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.21.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.21.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.21.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.21.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.21.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.21.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.21.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.21.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.22.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.22.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.22.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.22.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.22.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.22.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.22.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.22.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.22.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.22.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.22.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.22.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.23.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.23.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.23.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.23.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.23.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.23.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.23.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.23.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.23.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.23.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.23.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.23.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.24.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.24.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.24.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.24.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.24.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.24.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.24.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.24.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.24.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.24.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.24.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.24.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.25.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.25.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.25.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.25.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.25.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.25.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.25.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.25.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.25.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.25.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.25.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.25.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.26.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.26.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.26.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.26.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.26.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.26.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.26.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.26.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.26.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.26.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.26.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.26.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.27.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.27.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.27.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.27.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.27.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.27.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.27.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.27.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.27.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.27.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.27.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.27.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.28.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.28.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.28.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.28.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.28.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.28.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.28.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.28.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.28.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.28.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.28.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.28.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.29.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.29.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.29.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.29.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.29.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.29.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.29.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.29.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.29.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.29.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.29.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.29.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.ln_f.weight                  torch.float16       4,096\n",
      "transformer.ln_f.bias                    torch.float16       4,096\n"
     ]
    }
   ],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    print(f\"{name:40s} {p.dtype}  {p.numel():>10,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee4b2b",
   "metadata": {},
   "source": [
    "### 1. 8-bit 量化在 Transformers 里是怎么做的\n",
    "- **只有 `nn.Linear` 的权重张量被量化成 INT8**（1 字节）。  \n",
    "- **量化参数（scale/zero-point）和非线性层、bias、LayerNorm 等**仍保持 **float16**（2 字节）。  \n",
    "- 量化后的 `Linear` 模块被替换成 `bnb.nn.Int8Params`，但 **模块外壳仍注册为 `nn.Parameter`**，因此 `model.parameters()` 遍历到的**第一层往往就是非量化的 float16 参数**（例如 embed/LayerNorm/bias）。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 为什么 `.dtype` 不直接显示 `torch.int8`\n",
    "- **Int8Params 的 `.dtype` 被重载为原始反量化后的类型**（float16），方便框架内部做类型推断；  \n",
    "- 真实存储是 **1 字节 + 缩放系数**，可通过：\n",
    "  ```python\n",
    "  p = model.transformer.h[0].self_attn.q_proj.weight\n",
    "  print(p.dtype)          # torch.float16 （外壳类型）\n",
    "  print(p.CB.dtype)       # torch.int8   （真实量化数据）\n",
    "  print(p.SCB.dtype)      # torch.float16（缩放系数）\n",
    "  ```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "135ca9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c83d9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, param in enumerate(model.parameters()):\n",
    "#     param.requires_grad = False\n",
    "#     if param.ndim == 1: #把一维的小参数（bias、LayerNorm 等）单独升到 float32，保证数值稳定，其余权重保持原精度不动\n",
    "#         param.data = param.data.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc7605f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.word_embeddings.weight       torch.float16  1,027,604,480\n",
      "transformer.word_embeddings_layernorm.weight torch.float16       4,096\n",
      "transformer.word_embeddings_layernorm.bias torch.float16       4,096\n",
      "transformer.h.0.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.0.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.0.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.0.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.0.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.0.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.0.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.0.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.0.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.0.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.0.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.0.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.1.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.1.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.1.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.1.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.1.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.1.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.1.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.1.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.1.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.1.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.1.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.1.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.2.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.2.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.2.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.2.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.2.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.2.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.2.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.2.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.2.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.2.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.2.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.2.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.3.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.3.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.3.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.3.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.3.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.3.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.3.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.3.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.3.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.3.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.3.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.3.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.4.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.4.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.4.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.4.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.4.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.4.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.4.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.4.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.4.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.4.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.4.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.4.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.5.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.5.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.5.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.5.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.5.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.5.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.5.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.5.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.5.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.5.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.5.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.5.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.6.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.6.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.6.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.6.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.6.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.6.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.6.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.6.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.6.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.6.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.6.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.6.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.7.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.7.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.7.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.7.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.7.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.7.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.7.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.7.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.7.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.7.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.7.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.7.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.8.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.8.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.8.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.8.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.8.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.8.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.8.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.8.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.8.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.8.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.8.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.8.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.9.input_layernorm.weight   torch.float16       4,096\n",
      "transformer.h.9.input_layernorm.bias     torch.float16       4,096\n",
      "transformer.h.9.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.9.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.9.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.9.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.9.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.9.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.9.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.9.mlp.dense_h_to_4h.bias   torch.float16      16,384\n",
      "transformer.h.9.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.9.mlp.dense_4h_to_h.bias   torch.float16       4,096\n",
      "transformer.h.10.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.10.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.10.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.10.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.10.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.10.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.10.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.10.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.10.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.10.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.10.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.10.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.11.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.11.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.11.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.11.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.11.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.11.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.11.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.11.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.11.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.11.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.11.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.11.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.12.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.12.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.12.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.12.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.12.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.12.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.12.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.12.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.12.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.12.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.12.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.12.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.13.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.13.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.13.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.13.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.13.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.13.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.13.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.13.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.13.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.13.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.13.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.13.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.14.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.14.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.14.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.14.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.14.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.14.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.14.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.14.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.14.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.14.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.14.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.14.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.15.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.15.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.15.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.15.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.15.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.15.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.15.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.15.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.15.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.15.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.15.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.15.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.16.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.16.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.16.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.16.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.16.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.16.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.16.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.16.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.16.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.16.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.16.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.16.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.17.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.17.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.17.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.17.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.17.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.17.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.17.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.17.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.17.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.17.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.17.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.17.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.18.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.18.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.18.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.18.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.18.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.18.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.18.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.18.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.18.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.18.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.18.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.18.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.19.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.19.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.19.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.19.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.19.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.19.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.19.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.19.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.19.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.19.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.19.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.19.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.20.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.20.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.20.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.20.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.20.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.20.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.20.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.20.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.20.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.20.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.20.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.20.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.21.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.21.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.21.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.21.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.21.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.21.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.21.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.21.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.21.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.21.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.21.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.21.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.22.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.22.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.22.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.22.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.22.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.22.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.22.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.22.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.22.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.22.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.22.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.22.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.23.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.23.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.23.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.23.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.23.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.23.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.23.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.23.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.23.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.23.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.23.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.23.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.24.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.24.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.24.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.24.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.24.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.24.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.24.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.24.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.24.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.24.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.24.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.24.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.25.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.25.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.25.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.25.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.25.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.25.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.25.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.25.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.25.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.25.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.25.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.25.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.26.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.26.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.26.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.26.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.26.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.26.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.26.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.26.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.26.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.26.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.26.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.26.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.27.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.27.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.27.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.27.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.27.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.27.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.27.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.27.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.27.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.27.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.27.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.27.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.28.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.28.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.28.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.28.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.28.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.28.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.28.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.28.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.28.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.28.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.28.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.28.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.h.29.input_layernorm.weight  torch.float16       4,096\n",
      "transformer.h.29.input_layernorm.bias    torch.float16       4,096\n",
      "transformer.h.29.self_attention.query_key_value.weight torch.uint8  25,165,824\n",
      "transformer.h.29.self_attention.query_key_value.bias torch.float16      12,288\n",
      "transformer.h.29.self_attention.dense.weight torch.uint8   8,388,608\n",
      "transformer.h.29.self_attention.dense.bias torch.float16       4,096\n",
      "transformer.h.29.post_attention_layernorm.weight torch.float16       4,096\n",
      "transformer.h.29.post_attention_layernorm.bias torch.float16       4,096\n",
      "transformer.h.29.mlp.dense_h_to_4h.weight torch.uint8  33,554,432\n",
      "transformer.h.29.mlp.dense_h_to_4h.bias  torch.float16      16,384\n",
      "transformer.h.29.mlp.dense_4h_to_h.weight torch.uint8  33,554,432\n",
      "transformer.h.29.mlp.dense_4h_to_h.bias  torch.float16       4,096\n",
      "transformer.ln_f.weight                  torch.float16       4,096\n",
      "transformer.ln_f.bias                    torch.float16       4,096\n"
     ]
    }
   ],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    print(f\"{name:40s} {p.dtype}  {p.numel():>10,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00a61c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8388608, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].self_attention.dense.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951d4fd5",
   "metadata": {},
   "source": [
    "**微调超大模型时的标准“省显存 + 解冻”组合**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. `model.gradient_checkpointing_enable()`\n",
    "- **启用“梯度检查点”**（Gradient Checkpointing）。  \n",
    "- 前向时**不保存中间激活**，反向到对应层再临时重算一次前向，**显存换时间**。  \n",
    "- 通常可把显存占用 **砍 30-50 %**，适合 batch 较大或卡较小的情况。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `model.enable_input_require_grads()`\n",
    "- 把 **输入 embedding 层**的 `requires_grad` 设成 `True`。  \n",
    "- 默认冻结（`requires_grad=False`）时，**最前面一层的梯度不会回传**，也就无法：  \n",
    "  – 做 **LoRA/AdaLoRA**（需要输入梯度）  \n",
    "  – 对 **prompt-tuning / prefix-tuning** 等优化输入嵌入的场景  \n",
    "- 调用后，模型会强制给 `inputs_embeds` 打开梯度开关，保证这些算法能正常 backward。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 一句话总结\n",
    "> **“先开检查点省显存，再开输入梯度开关，让输入嵌入也能回传梯度，后面才能放心做 LoRA/prompt-tuning 等微调。”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "191e68cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()  \n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7251ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CastOutputToFloat(nn.Sequential):\n",
    "#     def forward(self, x):\n",
    "#         return super().forward(x).to(torch.float32) #将输出参数转为float32，权重参数不变\n",
    "# model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd3092",
   "metadata": {},
   "source": [
    "LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f37fcc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(mode):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _,param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"训练参数: {trainable_params} ||所有参数: {all_param} || 训练参数占比%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3f06b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2b49d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "066c3d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练参数: 1966080 ||所有参数: 4051083264 || 训练参数占比%: 0.04853220415071676\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f644f9",
   "metadata": {},
   "source": [
    "pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d90d39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Abirate/english_quotes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e948169f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['quote', 'author', 'tags'],\n",
       "        num_rows: 2508\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53593e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“Be yourself; everyone else is already taken.”',\n",
       " \"“I'm selfish, impatient and a little insecure. I make mistakes, I am out of control and at times hard to handle. But if you can't handle me at my worst, then you sure as hell don't deserve me at my best.”\",\n",
       " \"“Two things are infinite: the universe and human stupidity; and I'm not sure about the universe.”\",\n",
       " '“So many books, so little time.”',\n",
       " '“A room without books is like a body without a soul.”']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['quote'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "335948dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oscar Wilde',\n",
       " 'Marilyn Monroe',\n",
       " 'Albert Einstein',\n",
       " 'Frank Zappa',\n",
       " 'Marcus Tullius Cicero']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['author'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "796ed3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['be-yourself',\n",
       "  'gilbert-perreira',\n",
       "  'honesty',\n",
       "  'inspirational',\n",
       "  'misattributed-oscar-wilde',\n",
       "  'quote-investigator'],\n",
       " ['best', 'life', 'love', 'mistakes', 'out-of-control', 'truth', 'worst'],\n",
       " ['human-nature',\n",
       "  'humor',\n",
       "  'infinity',\n",
       "  'philosophy',\n",
       "  'science',\n",
       "  'stupidity',\n",
       "  'universe'],\n",
       " ['books', 'humor'],\n",
       " ['books', 'simile', 'soul']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['tags'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc28f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(row):\n",
    "    row['prediction'] = row['quote'] + ' ->: ' + str(row['tags'])\n",
    "    return row\n",
    "dataset['train'] = dataset['train'].map(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22b16f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['quote', 'author', 'tags', 'prediction'],\n",
       "        num_rows: 2508\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "622912b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.map(lambda x:tokenizer(x['prediction']), batched=True, remove_columns=['quote', 'author', 'tags', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5592108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 2508\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f16431b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f1120b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/home/jiangzirou/.local/lib/python3.8/site-packages/bitsandbytes/nn/modules.py:228: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n",
      "/home/jiangzirou/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jiangzirou/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/200 : < :, Epoch 0.06/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 84, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/peft/peft_model.py\", line 1644, in forward\n    return self.base_model(\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/peft/tuners/tuners_utils.py\", line 197, in forward\n    return self.model.forward(*args, **kwargs)\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/accelerate/hooks.py\", line 170, in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/transformers/models/bloom/modeling_bloom.py\", line 999, in forward\n    loss = loss_fct(\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py\", line 1188, in forward\n    return F.cross_entropy(input, target, weight=self.weight,\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/torch/nn/functional.py\", line 3104, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.54 GiB. GPU 0 has a total capacity of 23.64 GiB of which 324.44 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 18.80 GiB is allocated by PyTorch, and 4.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m      3\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \n\u001b[0;32m---> 17\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:186\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    185\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 186\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:201\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:109\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    107\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 109\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 84, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/peft/peft_model.py\", line 1644, in forward\n    return self.base_model(\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/peft/tuners/tuners_utils.py\", line 197, in forward\n    return self.model.forward(*args, **kwargs)\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/accelerate/hooks.py\", line 170, in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/transformers/models/bloom/modeling_bloom.py\", line 999, in forward\n    loss = loss_fct(\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py\", line 1188, in forward\n    return F.cross_entropy(input, target, weight=self.weight,\n  File \"/home/jiangzirou/.local/lib/python3.8/site-packages/torch/nn/functional.py\", line 3104, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.54 GiB. GPU 0 has a total capacity of 23.64 GiB of which 324.44 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 18.80 GiB is allocated by PyTorch, and 4.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    train_dataset=train_dataset['train'],\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=100,\n",
    "        max_steps=200,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"bloom-7b1-lora-0927\"\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19b1510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTroch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
