{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_senteces = ['today is not that bad', 'today is so bad', 'so good']\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "import os#环境代理设置\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiangzirou/data/miniconda3/envs/PyTroch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tokenizer(test_senteces, truncation=True, padding=True,return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2651, 2003, 2025, 2008, 2919,  102],\n",
       "        [ 101, 2651, 2003, 2061, 2919,  102,    0],\n",
       "        [ 101, 2061, 2204,  102,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'today', 'is', 'not', 'that', 'bad', '[SEP]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(input[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 102, 0, 101, 103]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids([op for op in tokenizer.special_tokens_map.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"finetuning_task\": \"sst-2\",\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"NEGATIVE\",\n",
       "    \"1\": \"POSITIVE\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"NEGATIVE\": 0,\n",
       "    \"POSITIVE\": 1\n",
       "  },\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.46.3\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[ 101, 2651, 2003, 2025, 2008, 2919,  102],\n",
      "        [ 101, 2651, 2003, 2061, 2919,  102,    0],\n",
      "        [ 101, 2061, 2204,  102,    0,    0,    0]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "def fc(**kwargs):\n",
    "    for k,v in kwargs.items():\n",
    "        print(k,v)\n",
    "fc(**input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-3.4620,  3.6118],\n",
      "        [ 4.7508, -3.7899],\n",
      "        [-4.1938,  4.5566]]), hidden_states=None, attentions=None)\n",
      "tensor([[8.4632e-04, 9.9915e-01],\n",
      "        [9.9980e-01, 1.9531e-04],\n",
      "        [1.5837e-04, 9.9984e-01]])\n",
      "tensor([1, 0, 1])\n",
      "['POSITIVE', 'NEGATIVE', 'POSITIVE']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**input)\n",
    "    print(outputs)\n",
    "    scores = F.softmax(outputs.logits,dim=-1)\n",
    "    print(scores)\n",
    "    labels = torch.argmax(scores,dim=-1)\n",
    "    print(labels)\n",
    "    labels = [model.config.id2label[id] for id in labels.tolist()]\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_input_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTroch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
